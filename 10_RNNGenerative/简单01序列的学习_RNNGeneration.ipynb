{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络序列生成\n",
    "\n",
    "在这节课中，我们主要学习了用神经网络生成符号序列。在这个notebook中，我们展示了让神经网络学习形如$0^n1^n$形式的上下文无关语法。\n",
    "\n",
    "然后在让网络生成这样的字符串，与此同时我们展示了如何使用PyTorch提供的RNN和LSTM的相关函数。在顺利完成学习任务后，我们还对RNN网络\n",
    "\n",
    "进行了解剖，我们考察RNN是如何完成记忆的。同时，我们比较了RNN与LSTM的记忆能力。\n",
    "\n",
    "本文件是集智学园http://campus.swarma.org 出品的“火炬上的深度学习”第VII课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入程序所需要的程序包\n",
    "\n",
    "#PyTorch用的包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "\n",
    "from collections import Counter #搜集器，可以让统计词频更简单\n",
    "\n",
    "#绘图、计算用的程序包\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import numpy as np\n",
    "#将图形直接显示出来\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、字符串的生成\n",
    "\n",
    "首先，我们生成大量的形如$0^n1^n$这样的字符串。为了让训练能够有更好的效果，我们故意生成了较多n比较小的字符串\n",
    "\n",
    "所有字符串都是如下的形式：30001112,300112,3012,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = []\n",
    "valid_set = []\n",
    "\n",
    "# 生成的样本数量\n",
    "samples = 2000\n",
    "\n",
    "# 训练样本中n的最大值\n",
    "sz = 10\n",
    "# 定义不同n的权重，我们按照10:6:4:3:1:1...来配置字符串生成中的n=1,2,3,4,5,...\n",
    "probability = 1.0 * np.array([10, 6, 4, 3, 1, 1, 1, 1, 1, 1])\n",
    "# 保证n的最大值为sz\n",
    "probability = probability[ : sz]\n",
    "# 归一化，将权重变成概率\n",
    "probability = probability / sum(probability)\n",
    "\n",
    "# 开始生成samples这么多个样本\n",
    "for m in range(samples):\n",
    "    # 对于每一个生成的字符串，随机选择一个n，n被选择的权重被记录在probability中\n",
    "    n = np.random.choice(range(1, sz + 1), p = probability)\n",
    "    # 生成这个字符串，用list的形式完成记录\n",
    "    inputs = [0] * n + [1] * n\n",
    "    # 在最前面插入3表示起始字符，2插入尾端表示终止字符\n",
    "    inputs.insert(0, 3)\n",
    "    inputs.append(2)\n",
    "    train_set.append(inputs) #将生成的字符串加入到train_set训练集中\n",
    "    \n",
    "# 再生成samples/10的校验样本\n",
    "for m in range(samples // 10):\n",
    "    n = np.random.choice(range(1, sz + 1), p = probability)\n",
    "    inputs = [0] * n + [1] * n\n",
    "    inputs.insert(0, 3)\n",
    "    inputs.append(2)\n",
    "    valid_set.append(inputs)\n",
    "\n",
    "# 再生成若干n超大的校验样本\n",
    "for m in range(2):\n",
    "    n = sz + m\n",
    "    inputs = [0] * n + [1] * n\n",
    "    inputs.insert(0, 3)\n",
    "    inputs.append(2)\n",
    "    valid_set.append(inputs)\n",
    "np.random.shuffle(valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、用RNN来进行学习\n",
    "\n",
    "首先，我们先让RNN来对这些字符串进行学习，然后再去实验LSTM的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 定义简单RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现一个简单的RNN模型\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers = 1):\n",
    "        # 定义\n",
    "        super(SimpleRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # 一个embedding层\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # PyTorch的RNN层，batch_first标志可以让输入的张量的第一个维度表示batch指标\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first = True)\n",
    "        # 输出的全链接层\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        # 最后的logsoftmax层\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # 运算过程\n",
    "        # 先进行embedding层的计算，它可以把一个数值先转化为one-hot向量，再把这个向量转化为一个hidden_size维的向量\n",
    "        # input的尺寸为：batch_size, num_step, data_dim\n",
    "        x = self.embedding(input)\n",
    "        # 从输入到隐含层的计算\n",
    "        # x的尺寸为：batch_size, num_step, hidden_size\n",
    "        output, hidden = self.rnn(x, hidden)\n",
    "        # 从输出output中取出最后一个时间步的数值，注意output输出包含了所有时间步的结果,\n",
    "        # output输出尺寸为：batch_size, num_step, hidden_size\n",
    "        output = output[:,-1,:]\n",
    "        # output尺寸为：batch_size, hidden_size\n",
    "        # 喂入最后一层全链接网络\n",
    "        output = self.fc(output)\n",
    "        # output尺寸为：batch_size, output_size\n",
    "        # softmax函数\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        # 对隐含单元的初始化\n",
    "        # 注意尺寸是： layer_size, batch_size, hidden_size\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、训练神经网络\n",
    "\n",
    "我们进行了三重循环，Epoch作为第一重循环，然后对每个train_set中的字符串做第二重循环，最后是对每一个字符串中的每一个字符做循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成一个最简化的RNN，输入size为4，可能值为0,1,2,3，输出size为3，可能值为0,1,2\n",
    "rnn = SimpleRNN(input_size = 4, hidden_size = 2, output_size = 3)\n",
    "criterion = torch.nn.NLLLoss() #交叉熵损失函数\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr = 0.001) #Adam优化算法\n",
    "\n",
    "\n",
    "#重复进行50次试验\n",
    "num_epoch = 5\n",
    "results = []\n",
    "for epoch in range(num_epoch):\n",
    "    train_loss = 0\n",
    "    # 对train_set中的数据进行随机洗牌，以保证每个epoch得到的训练顺序都不一样。\n",
    "    np.random.shuffle(train_set)\n",
    "    # 对train_set中的数据进行循环\n",
    "    for i, seq in enumerate(train_set):\n",
    "        loss = 0\n",
    "        hidden = rnn.initHidden()  #初始化隐含层神经元\n",
    "        # 对每一个序列的所有字符进行循环\n",
    "        for t in range(len(seq) - 1):\n",
    "            #当前字符作为输入，下一个字符作为标签\n",
    "            x = torch.LongTensor([seq[t]]).unsqueeze(0)\n",
    "            # x尺寸：batch_size = 1, time_steps = 1, data_dimension = 1\n",
    "            y = torch.LongTensor([seq[t + 1]])\n",
    "            # y尺寸：batch_size = 1, data_dimension = 1\n",
    "            output, hidden = rnn(x ,hidden) #RNN输出\n",
    "            # output尺寸：batch_size, output_size = 3\n",
    "            # hidden尺寸：layer_size =1, batch_size=1, hidden_size\n",
    "            loss += criterion(output, y) #计算损失函数\n",
    "        loss = 1.0 * loss / len(seq) #计算每字符的损失数值\n",
    "        optimizer.zero_grad() # 梯度清空\n",
    "        loss.backward() #反向传播，设置retain_variables\n",
    "        optimizer.step() #一步梯度下降\n",
    "        train_loss += loss #累积损失函数值\n",
    "        # 把结果打印出来\n",
    "        if i > 0 and i % 500 == 0:\n",
    "            print('第{}轮, 第{}个，训练Loss:{:.2f}'.format(epoch,\n",
    "                                                    i,\n",
    "                                                    train_loss.data.numpy() / i\n",
    "                                                   ))\n",
    "            \n",
    "    # 在校验集上测试\n",
    "\n",
    "    valid_loss = 0\n",
    "    errors = 0\n",
    "    show_out = ''\n",
    "    for i, seq in enumerate(valid_set):\n",
    "        # 对每一个valid_set中的字符串做循环\n",
    "        loss = 0\n",
    "        outstring = ''\n",
    "        targets = ''\n",
    "        diff = 0\n",
    "        hidden = rnn.initHidden() #初始化隐含层神经元\n",
    "        for t in range(len(seq) - 1):\n",
    "            # 对每一个字符做循环\n",
    "            x = torch.tensor([seq[t]], dtype = torch.long).unsqueeze(0)\n",
    "            # x尺寸：batch_size = 1, time_steps = 1, data_dimension = 1\n",
    "            y = torch.tensor([seq[t + 1]], dtype = torch.long)\n",
    "            # y尺寸：batch_size = 1, data_dimension = 1\n",
    "            output, hidden = rnn(x, hidden)\n",
    "            # output尺寸：batch_size, output_size = 3\n",
    "            # hidden尺寸：layer_size =1, batch_size=1, hidden_size\n",
    "            mm = torch.max(output, 1)[1][0] #以概率最大的元素作为输出\n",
    "            outstring += str(mm.data.numpy()) #合成预测的字符串\n",
    "            targets += str(y.data.numpy()[0]) #合成目标字符串\n",
    "            loss += criterion(output, y) #计算损失函数\n",
    "            diff += 1 - mm.eq(y).data.numpy()[0] #计算模型输出字符串与目标字符串之间差异的字符数量\n",
    "        loss = 1.0 * loss / len(seq)\n",
    "        valid_loss += loss #累积损失函数值\n",
    "        errors += diff #计算累积错误数\n",
    "        if np.random.rand() < 0.1:\n",
    "            #以0.1概率记录一个输出字符串\n",
    "            show_out = outstring + '\\n' + targets\n",
    "    # 打印结果\n",
    "    print(output[0][2].data.numpy())\n",
    "    print('第{}轮, 训练Loss:{:.2f}, 校验Loss:{:.2f}, 错误率:{:.2f}'.format(epoch, \n",
    "                                                               train_loss.data.numpy() / len(train_set),\n",
    "                                                               valid_loss.data.numpy()/ len(valid_set),\n",
    "                                                               1.0 * errors / len(valid_set)\n",
    "                                                              ))\n",
    "    print(show_out)\n",
    "    results.append([train_loss.data.numpy() / len(train_set), \n",
    "                    valid_loss.data.numpy() / len(train_set),\n",
    "                   1.0 * errors / len(valid_set)\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存、提取模型（为展示用）\n",
    "#torch.save(rnn,'rnn.mdl')\n",
    "#rnn = torch.load('rnn.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 测试结果好坏\n",
    "\n",
    "我们让n从0循环到20，考察模型预测的结果如何\n",
    "\n",
    "只有当模型能够预测出最后一个1后面应该跟2（字串结束字符）才算预测正确，也就意味着模型记忆住了n这个数字。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(20):\n",
    "    \n",
    "    inputs = [0] * n + [1] * n\n",
    "    inputs.insert(0, 3)\n",
    "    inputs.append(2)\n",
    "    outstring = ''\n",
    "    targets = ''\n",
    "    diff = 0\n",
    "    hiddens = []\n",
    "    hidden = rnn.initHidden()\n",
    "    for t in range(len(inputs) - 1):\n",
    "        x = torch.tensor([inputs[t]], dtype = torch.long).unsqueeze(0)\n",
    "        # x尺寸：batch_size = 1, time_steps = 1, data_dimension = 1\n",
    "        y = torch.tensor([inputs[t + 1]], dtype = torch.long)\n",
    "        # y尺寸：batch_size = 1, data_dimension = 1\n",
    "        output, hidden = rnn(x, hidden)\n",
    "        # output尺寸：batch_size, output_size = 3\n",
    "        # hidden尺寸：layer_size =1, batch_size=1, hidden_size\n",
    "        hiddens.append(hidden.data.numpy()[0][0])\n",
    "        #mm = torch.multinomial(output.view(-1).exp())\n",
    "        mm = torch.max(output, 1)[1][0]\n",
    "        outstring += str(mm.data.numpy())\n",
    "        targets += str(y.data.numpy()[0])\n",
    "\n",
    "        diff += 1 - mm.eq(y).data.numpy()[0]\n",
    "    # 打印出每一个生成的字符串和目标字符串\n",
    "    print(outstring)\n",
    "    print(targets)\n",
    "    print('Diff:{}'.format(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 解剖RNN\n",
    "\n",
    "接下来，我们尝试分析一个被训练好的RNN是如何完成对下一个字符进行预测的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先，选择特定的n，将生成的字符串喂给RNN，并让它完成预测下一个的任务，同时，我们记录下来它的隐含层神经元的每一步状态。\n",
    "n = 2\n",
    "inputs = [0] * n + [1] * n\n",
    "inputs.insert(0, 3)\n",
    "inputs.append(2)\n",
    "outstring = ''\n",
    "targets = ''\n",
    "diff = 0\n",
    "hiddens = []\n",
    "hidden = rnn.initHidden()\n",
    "for t in range(len(inputs) - 1):\n",
    "    x = torch.tensor([inputs[t]], dtype = torch.long).unsqueeze(0)\n",
    "    y = torch.tensor([inputs[t + 1]], dtype = torch.long)\n",
    "    output, hidden = rnn(x, hidden)\n",
    "    hiddens.append(hidden.data.numpy()[0][0]) #将每一步RNN的隐含层神经元的信息都记录下来\n",
    "    #mm = torch.multinomial(output.view(-1).exp())\n",
    "    mm = torch.max(output, 1)[1][0]\n",
    "    outstring += str(mm.data.numpy())\n",
    "    targets += str(y.data.numpy()[0])\n",
    "\n",
    "    diff += 1 - mm.eq(y).data.numpy()[0]\n",
    "print(n)\n",
    "print(outstring)\n",
    "print(targets)\n",
    "print('Diff:{}'.format(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 将两个隐含单元随时间步的变化情况绘制成曲线\n",
    "ab =np.transpose(hiddens)\n",
    "\n",
    "# rc('font',**{'family':'sans-serif','sans-serif':['Helvetica'], 'size': 22})\n",
    "# rc('text', usetex=True)\n",
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "for i in range(len(ab)):\n",
    "    plt.plot(ab[i], 'o-', label = '$h_' + str(i + 1) +'$' )\n",
    "plt.xlabel(r'$t$')\n",
    "plt.ylabel(r'$h_1,h_2$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 将隐含层单元的情况重新绘制到状态空间中，其中h1为横坐标，h2为纵坐标。\n",
    "# 同时，我们将最后一层神经元所对应的分类线也给画出来\n",
    "X_reduced = np.array(hiddens)\n",
    "plt.figure(figsize = (10 , 8))\n",
    "plt.plot(X_reduced[: n, 0], X_reduced[: n, 1], 'o-')\n",
    "plt.plot(X_reduced[n :, 0], X_reduced[n :, 1], 's-')\n",
    "xx = np.linspace(-0.5, 1, 100)\n",
    "\n",
    "# 将神经网络的全链接层权重和bias信息提取出来\n",
    "dic = dict(rnn.named_parameters()) #提取出来这个集合\n",
    "weights = dic['fc.weight'].data.numpy()\n",
    "bias = dic['fc.bias'].data.numpy()\n",
    "\n",
    "# 绘制分类线\n",
    "yy = - ((weights[0][0] - weights[1][0]) * xx  + (bias[0] - bias[1])) / (weights[0][1] - weights[1][1])\n",
    "plt.plot(xx, yy, ':', label = '0/1')\n",
    "yy = - ((weights[1][0] - weights[2][0]) * xx  + (bias[1] - bias[2])) / (weights[1][1] - weights[2][1])\n",
    "plt.plot(xx, yy, ':', label = '1/2')\n",
    "yy = - ((weights[0][0] - weights[2][0]) * xx  + (bias[0] - bias[2])) / (weights[0][1] - weights[2][1])\n",
    "plt.plot(xx, yy, ':', label = '0/2')\n",
    "\n",
    "zhfont1 = matplotlib.font_manager.FontProperties(size=14)\n",
    "for i in range(len(hiddens)):\n",
    "    plt.text(X_reduced[i, 0], X_reduced[i, 1] + 0.1, '$t=' + str(i) + '$', fontproperties = zhfont1, alpha = 1)\n",
    "plt.xlabel(r'$h_1$')\n",
    "plt.ylabel(r'$h_2$')\n",
    "plt.ylim([-2, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用训练好的神经模型自由生成字符串\n",
    "input = 3\n",
    "outstring = '' + str(input)\n",
    "targets = ''\n",
    "diff = 0\n",
    "hiddens = []\n",
    "hidden = rnn.initHidden()\n",
    "t = 0 #记录执行次数，最多执行1000步。\n",
    "while t < 1000:\n",
    "    x = torch.tensor([input], dtype = torch.long).unsqueeze(0)\n",
    "    output, hidden = rnn(x, hidden)\n",
    "    mm = torch.multinomial(output.view(-1).exp(), num_samples = 1 )\n",
    "    outstring += str(mm.data.numpy()[0])\n",
    "    input = int(mm.data.numpy()[0])\n",
    "    t += 1\n",
    "    if input == 2:\n",
    "        break\n",
    "print(t)\n",
    "print(outstring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、用LSTM来完成字符预测\n",
    "\n",
    "接下来，我们用一个单一隐含层单元的LSTM来完成完全相同的任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 实现一个LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个手动实现的LSTM模型，除了初始化隐含但愿部分，所有代码基本与SimpleRNN相同\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers = 1):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # 一个embedding层\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        # 隐含层内部的相互链接\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        # 先进行embedding层的计算，它可以把一个\n",
    "        # x的尺寸：batch_size, len_seq, input_size\n",
    "        x = self.embedding(input)\n",
    "        # x的尺寸：batch_size, len_seq, hidden_size\n",
    "        # 从输入到隐含层的计算\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        # output的尺寸：batch_size, len_seq, hidden_size\n",
    "        # hidden: (layer_size, batch_size, hidden_size),(layer_size, batch_size,hidden_size)\n",
    "        output = output[:,-1,:]\n",
    "        # output的尺寸：batch_size, hidden_size\n",
    "        output = self.fc(output)\n",
    "        # output的尺寸：batch_size, output_size\n",
    "        # softmax函数\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    " \n",
    "    def initHidden(self):\n",
    "        # 对隐含单元的初始化\n",
    "        # 注意尺寸是： layer_size, batch_size, hidden_size\n",
    "        # 对隐单元的初始化\n",
    "        # 对引单元输出的初始化，全0.\n",
    "        # 注意hidden和cell的维度都是layers,batch_size,hidden_size\n",
    "        hidden = torch.zeros(self.num_layers, 1, self.hidden_size)\n",
    "        # 对隐单元内部的状态cell的初始化，全0\n",
    "        cell = torch.zeros(self.num_layers, 1, self.hidden_size)\n",
    "        return (hidden, cell)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 训练SimpleLSTM\n",
    "\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第III课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = SimpleLSTM(input_size = 4, hidden_size = 1, output_size = 3, num_layers = 1)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr = 0.001)\n",
    "\n",
    "num_epoch = 5\n",
    "results = []\n",
    "\n",
    "# 开始训练循环\n",
    "for epoch in range(num_epoch):\n",
    "    train_loss = 0\n",
    "    np.random.shuffle(train_set)\n",
    "    # 开始所有训练数据的循环\n",
    "    for i, seq in enumerate(train_set):\n",
    "        loss = 0\n",
    "        hidden = lstm.initHidden()\n",
    "        # 开始每一个字符的循环\n",
    "        for t in range(len(seq) - 1):\n",
    "            x = torch.tensor([seq[t]], dtype = torch.long).unsqueeze(0)\n",
    "            # x的尺寸：batch_size, len_seq, hidden_size\n",
    "            y = torch.tensor([seq[t + 1]], dtype = torch.long)\n",
    "            # y的尺寸：batch_size, data_dimension\n",
    "            output, hidden = lstm(x, hidden)\n",
    "            # output的尺寸：batch_size, data_dimension\n",
    "            # hidden: (layer_size, batch_size, hidden_size),(layer_size, batch_size,hidden_size)\n",
    "            loss += criterion(output, y)\n",
    "        loss = 1.0 * loss / len(seq)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward(retain_graph = True)\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "        if i > 0 and i % 500 == 0:\n",
    "            print('第{}轮, 第{}个，训练Loss:{:.2f}'.format(epoch,\n",
    "                                                    i,\n",
    "                                                    train_loss.data.numpy() / i\n",
    "                                                   ))\n",
    "            \n",
    "            \n",
    "    # 在校验集上跑结果\n",
    "    valid_loss = 0\n",
    "    errors = 0\n",
    "    show_out = ''\n",
    "    for i, seq in enumerate(valid_set):\n",
    "        loss = 0\n",
    "        outstring = ''\n",
    "        targets = ''\n",
    "        diff = 0\n",
    "        hidden = lstm.initHidden()\n",
    "        for t in range(len(seq) - 1):\n",
    "            x = torch.tensor([seq[t]], dtype = torch.long).unsqueeze(0)\n",
    "            # x的尺寸：batch_size, len_seq, hidden_size\n",
    "            y = torch.tensor([seq[t + 1]], dtype = torch.long)\n",
    "            # y的尺寸：batch_size, data_dimension\n",
    "            output, hidden = lstm(x, hidden)\n",
    "            # output的尺寸：batch_size, data_dimension\n",
    "            # hidden: (layer_size, batch_size, hidden_size),(layer_size, batch_size,hidden_size)\n",
    "            mm = torch.max(output, 1)[1][0]\n",
    "            outstring += str(mm.data.numpy())\n",
    "            targets += str(y.data.numpy()[0])\n",
    "            loss += criterion(output, y)\n",
    "            \n",
    "            diff += 1 - mm.eq(y).data.numpy()[0]\n",
    "        loss = 1.0 * loss / len(seq)\n",
    "        valid_loss += loss\n",
    "        errors += diff\n",
    "        if np.random.rand() < 0.1:\n",
    "            show_out = outstring + '\\n' + targets\n",
    "    print(output[0][2].data.numpy())\n",
    "    print('第{}轮, 训练Loss:{:.2f}, 校验Loss:{:.2f}, 错误率:{:.2f}'.format(epoch, \n",
    "                                                               train_loss.data.numpy() / len(train_set),\n",
    "                                                               valid_loss.data.numpy() / len(valid_set),\n",
    "                                                               1.0 * errors / len(valid_set)\n",
    "                                                              ))\n",
    "    print(show_out)\n",
    "    results.append([train_loss.data.numpy() / len(train_set), \n",
    "                    valid_loss.data.numpy() / len(train_set),\n",
    "                   1.0 * errors / len(valid_set)\n",
    "                   ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文件是集智学园http://campus.swarma.org 出品的“火炬上的深度学习”第III课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存、提取模型（为展示用）\n",
    "#torch.save(lstm,'lstm.mdl')\n",
    "#lstm = torch.load('lstm.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让n取0到50，看SimpleLSTM是否能够成功预测下一个字符\n",
    "for n in range(50):\n",
    "    \n",
    "    inputs = [0] * n + [1] * n\n",
    "    inputs.insert(0, 3)\n",
    "    inputs.append(2)\n",
    "    outstring = ''\n",
    "    targets = ''\n",
    "    diff = 0\n",
    "    hiddens = []\n",
    "    hidden = lstm.initHidden()\n",
    "    for t in range(len(inputs) - 1):\n",
    "        x = torch.tensor([inputs[t]], dtype = torch.long).unsqueeze(0)\n",
    "        y = torch.tensor([inputs[t + 1]], dtype = torch.long)\n",
    "        output, hidden = lstm(x, hidden)\n",
    "        \n",
    "        mm = torch.max(output, 1)[1][0]\n",
    "        outstring += str(mm.data.numpy())\n",
    "        targets += str(y.data.numpy()[0])\n",
    "\n",
    "        diff += 1 - mm.eq(y).data.numpy()[0]\n",
    "    print(n)\n",
    "    print(outstring)\n",
    "    print(targets)\n",
    "    print('Diff:{}'.format(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(lstm, input, hidden):\n",
    "        \n",
    "    # 该函数的作用是计算出在给定的输入input下，所有门的打开关闭情况、最后的分类结果\n",
    "    x = lstm.embedding(input)\n",
    "    output, hidden = lstm.lstm(x, hidden)\n",
    "    output = output[:,-1,:]\n",
    "\n",
    "    output = lstm.fc(output)\n",
    "    # softmax函数\n",
    "    output = lstm.softmax(output)\n",
    "    dic = dict(lstm.lstm.named_parameters())\n",
    "    \n",
    "    #提取更个中间层的权重\n",
    "    weight_ih = dic['weight_ih_l0']\n",
    "    weight_hh = dic['weight_hh_l0']\n",
    "    bias_ih = dic['bias_ih_l0']\n",
    "    bias_hh = dic['bias_hh_l0']\n",
    "    #gt为一个中间变量\n",
    "    all_gates = weight_ih.mm(x[0])\n",
    "    gt = torch.tanh(all_gates[2])\n",
    "    all_gates = torch.sigmoid(all_gates)\n",
    "    all_gates[2] = gt\n",
    "\n",
    "    return all_gates, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 解剖LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固定n＝20，计算每一步LSTM的内部状态值，并记录下来\n",
    "n = 20\n",
    "inputs = [0] * n + [1] * n\n",
    "inputs.insert(0, 3)\n",
    "inputs.append(2)\n",
    "outstring = ''\n",
    "targets = ''\n",
    "diff = 0\n",
    "hiddens = []\n",
    "gates = []\n",
    "outputs = []\n",
    "hidden = lstm.initHidden()\n",
    "dicts = dict(lstm.named_parameters())\n",
    "for t in range(len(inputs) - 1):\n",
    "    x = torch.tensor([inputs[t]], dtype = torch.long).unsqueeze(0)\n",
    "    y = torch.tensor([inputs[t + 1]], dtype = torch.long)\n",
    "    gate, out = calculate(lstm, x, hidden)\n",
    "    outputs.append(np.exp(out.data.numpy()[0]))\n",
    "    output, hidden = lstm(x, hidden)\n",
    "    gates.append(gate.data.numpy().reshape(-1))\n",
    "    hiddens.append(hidden)\n",
    "    mm = torch.max(output, 1)[1][0]\n",
    "    outstring += str(mm.data.numpy())\n",
    "    targets += str(y.data.numpy()[0])\n",
    "\n",
    "    diff += 1 - mm.eq(y).data.numpy()[0]\n",
    "print(n)\n",
    "print(outstring)\n",
    "print(targets)\n",
    "print('Diff:{}'.format(diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 绘制曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制输出层随时间变化的曲线\n",
    "outputs = np.array(outputs)\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica'], 'size': 22})\n",
    "rc('text', usetex=True)\n",
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "plt.plot(outputs, 'o-')\n",
    "plt.xlabel(r'$t$')\n",
    "plt.ylabel(r'$o_i$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制隐含层cell与输出的数值随时间变化的曲线\n",
    "states = []\n",
    "cells = []\n",
    "\n",
    "for h in hiddens:\n",
    "    states.append(h[0].data.numpy()[0][0][0])\n",
    "    cells.append(h[1].data.numpy()[0][0][0])\n",
    "# rc('font',**{'family':'sans-serif','sans-serif':['Helvetica'], 'size': 22})\n",
    "# rc('text', usetex=True)\n",
    "plt.figure(figsize = (10, 8))\n",
    "\n",
    "plt.plot(states, 'o-', label = '$h$')\n",
    "plt.plot(cells, 's-', label = '$c$')\n",
    "plt.xlabel(r'$t$')\n",
    "plt.ylabel(r'$h,c$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制各个门控单元的取值状态\n",
    "gates = np.array(gates)\n",
    "\n",
    "# rc('font',**{'family':'sans-serif','sans-serif':['Helvetica'], 'size': 22})\n",
    "# rc('text', usetex=True)\n",
    "plt.figure(figsize = (10, 8))\n",
    "txts = ['i','f','c','o']\n",
    "for i in range(len(gates[0])):\n",
    "    if i != 5:\n",
    "        plt.plot(gates[:, i], 'o-', label = '$' + txts[i] + '$')\n",
    "plt.xlabel(r'$t$')\n",
    "plt.ylabel(r'$i,f,o$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "本文件是集智学园http://campus.swarma.org 出品的“火炬上的深度学习”第III课的配套源代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
