{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度强化学习－用卷积神经网络实现AI玩Flappy Bird游戏\n",
    "\n",
    "本节课我们结合Flappy bird游戏，详细讲述了深度强化学习原理，以及如何训练一个神经网络来玩儿游戏\n",
    "\n",
    "整个代码包括了利用PyGame包实现一个Flappy Bird游戏，卷积神经网络的定义与实现，以及深度强化学习算法。\n",
    "\n",
    "本程序参考了AI玩Flappy Bird的TensorFlow版本：https://github.com/yenchenlin/DeepLearningFlappyBird\n",
    "\n",
    "本文件是集智AI学园http://campus.swarma.org 出品的“火炬上的深度学习”第X课的配套源代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、PyGAME实现Flappy Bird游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这部分中，我们调用PyGame包实现了一个Flappy Bird游戏。通过PyGame，我们可以非常方便的加载图片、音频，来快速实现小游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 加载游戏所需的必要资源"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载游戏中的所有资源，包括图片以及音频\n",
    "# 调用PyGame包，关于该包的安装，请参看：http://www.pygame.org/wiki/GettingStarted\n",
    "import pygame\n",
    "\n",
    "# 需要获取操作系统类型，故而调用sys包\n",
    "import sys\n",
    "def load():\n",
    "    # 加载各类资源的函数\n",
    "    # 精灵在不同状态下的图片\n",
    "    PLAYER_PATH = (\n",
    "            'assets/sprites/redbird-upflap.png',\n",
    "            'assets/sprites/redbird-midflap.png',\n",
    "            'assets/sprites/redbird-downflap.png'\n",
    "    )\n",
    "\n",
    "    # 背景图地址\n",
    "    BACKGROUND_PATH = 'assets/sprites/background-black.png'\n",
    "\n",
    "    # 管道图片所在的地址\n",
    "    PIPE_PATH = 'assets/sprites/pipe-green.png'\n",
    "\n",
    "    IMAGES, SOUNDS, HITMASKS = {}, {}, {}\n",
    "\n",
    "    # 加载成绩数字所需的图片\n",
    "    IMAGES['numbers'] = (\n",
    "        pygame.image.load('assets/sprites/0.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/1.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/2.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/3.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/4.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/5.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/6.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/7.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/8.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/9.png').convert_alpha()\n",
    "    )\n",
    "\n",
    "    # 加载地面的图片\n",
    "    IMAGES['base'] = pygame.image.load('assets/sprites/base.png').convert_alpha()\n",
    "\n",
    "    # 加载声音文件（在不同的系统中，声音文件扩展名不同）\n",
    "    if 'win' in sys.platform:\n",
    "        soundExt = '.wav'\n",
    "    else:\n",
    "        soundExt = '.ogg'\n",
    "\n",
    "    SOUNDS['die']    = pygame.mixer.Sound('assets/audio/die' + soundExt)\n",
    "    SOUNDS['hit']    = pygame.mixer.Sound('assets/audio/hit' + soundExt)\n",
    "    SOUNDS['point']  = pygame.mixer.Sound('assets/audio/point' + soundExt)\n",
    "    SOUNDS['swoosh'] = pygame.mixer.Sound('assets/audio/swoosh' + soundExt)\n",
    "    SOUNDS['wing']   = pygame.mixer.Sound('assets/audio/wing' + soundExt)\n",
    "\n",
    "    # 加载背景图\n",
    "    IMAGES['background'] = pygame.image.load(BACKGROUND_PATH).convert()\n",
    "\n",
    "    # s加载精灵图\n",
    "    IMAGES['player'] = (\n",
    "        pygame.image.load(PLAYER_PATH[0]).convert_alpha(),\n",
    "        pygame.image.load(PLAYER_PATH[1]).convert_alpha(),\n",
    "        pygame.image.load(PLAYER_PATH[2]).convert_alpha(),\n",
    "    )\n",
    "\n",
    "    # 加载水管\n",
    "    IMAGES['pipe'] = (\n",
    "        pygame.transform.rotate(\n",
    "            pygame.image.load(PIPE_PATH).convert_alpha(), 180),\n",
    "        pygame.image.load(PIPE_PATH).convert_alpha(),\n",
    "    )\n",
    "\n",
    "    # 获得水管的蒙板\n",
    "    HITMASKS['pipe'] = (\n",
    "        getHitmask(IMAGES['pipe'][0]),\n",
    "        getHitmask(IMAGES['pipe'][1]),\n",
    "    )\n",
    "\n",
    "    # 玩家的蒙板\n",
    "    HITMASKS['player'] = (\n",
    "        getHitmask(IMAGES['player'][0]),\n",
    "        getHitmask(IMAGES['player'][1]),\n",
    "        getHitmask(IMAGES['player'][2]),\n",
    "    )\n",
    "\n",
    "    #返回了三个字典，每个字典的值分别存储图像、声音和蒙板\n",
    "    return IMAGES, SOUNDS, HITMASKS\n",
    "\n",
    "def getHitmask(image):\n",
    "    \"\"\"根据图像的alpha，获得蒙板\"\"\"\n",
    "    #所谓蒙板就是指将图像中的主体从整个图像中抠出来的技术，从而方便与其它的对象合成到一起\n",
    "    #蒙板用一个boolean类型的列表来存储\n",
    "    mask = []\n",
    "    for x in range(image.get_width()):\n",
    "        mask.append([])\n",
    "        for y in range(image.get_height()):\n",
    "            mask[x].append(bool(image.get_at((x,y))[3]))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 实现Flappy Bird的游戏逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 加载程序所需的包\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import pygame\n",
    "import pygame.surfarray as surfarray\n",
    "from pygame.locals import *\n",
    "from itertools import cycle\n",
    "\n",
    "FPS = 30 #帧率\n",
    "SCREENWIDTH  = 288 #屏幕的宽度\n",
    "SCREENHEIGHT = 512 #屏幕的高度\n",
    "\n",
    "pygame.init() #游戏初始化\n",
    "FPSCLOCK = pygame.time.Clock() #定义程序时钟\n",
    "SCREEN = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT)) #定义屏幕对象\n",
    "pygame.display.set_caption('Flappy Bird') #设定窗口名称\n",
    "\n",
    "IMAGES, SOUNDS, HITMASKS = load() #加载游戏资源\n",
    "PIPEGAPSIZE = 100 # 定义两个水管之间的宽度\n",
    "BASEY = SCREENHEIGHT * 0.79 #设定基地的高度\n",
    "\n",
    "# 设定小鸟属性：宽度、高度等\n",
    "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
    "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
    "\n",
    "# 设定水管属性：高度、宽度\n",
    "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
    "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
    "\n",
    "#背景宽度\n",
    "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
    "\n",
    "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
    "\n",
    "# 游戏模型类\n",
    "class GameState:\n",
    "    def __init__(self):\n",
    "        # 初始化\n",
    "        # 初始成绩、玩家索引、循环迭代都为0\n",
    "        self.score = self.playerIndex = self.loopIter = 0\n",
    "        \n",
    "        #设定玩家的初始位置\n",
    "        self.playerx = int(SCREENWIDTH * 0.2)\n",
    "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
    "        self.basex = 0\n",
    "        # 地面的初始移位\n",
    "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
    "\n",
    "        # 生成两个随机的水管\n",
    "        newPipe1 = getRandomPipe()\n",
    "        newPipe2 = getRandomPipe()\n",
    "        \n",
    "        # 设定初始水管的位置x，y坐标\n",
    "        self.upperPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
    "        ]\n",
    "        self.lowerPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
    "        ]\n",
    "\n",
    "        # 定义玩家的属性\n",
    "        self.pipeVelX = -4\n",
    "        self.playerVelY    =  0    # 小鸟在y轴上的速度，初始设置维playerFlapped\n",
    "        self.playerMaxVelY =  10   # Y轴上的最大速度, 也就是最大的下降速度\n",
    "        self.playerMinVelY =  -8   # Y轴向上的最大速度\n",
    "        self.playerAccY    =   1   # 小鸟往下落的加速度\n",
    "        self.playerFlapAcc =  -9   # 扇动翅膀的加速度\n",
    "        self.playerFlapped = False # 玩家是否煽动了翅膀\n",
    "\n",
    "    def frame_step(self, input_actions):\n",
    "        # input_actions是一个行动数组，分别存储了0或者1两个动作的激活情况\n",
    "        # 游戏每一帧的循环\n",
    "        pygame.event.pump()\n",
    "\n",
    "        # 每一步的默认回报\n",
    "        reward = 0.1\n",
    "        terminal = False\n",
    "\n",
    "        # 限定每一帧只能做一个动作\n",
    "        if sum(input_actions) != 1:\n",
    "            raise ValueError('Multiple input actions!')\n",
    "\n",
    "        # input_actions[0] == 1: 对应什么都不做\n",
    "        # input_actions[1] == 1: 对应小鸟煽动了翅膀\n",
    "        if input_actions[1] == 1:\n",
    "            # 小鸟煽动翅膀向上\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "                #SOUNDS['wing'].play()\n",
    "\n",
    "        # 检查是否通过了管道，如果通过，则增加成绩\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        for pipe in self.upperPipes:\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n",
    "                self.score += 1\n",
    "                #SOUNDS['point'].play()\n",
    "                reward = 1\n",
    "\n",
    "        # playerIndex轮换\n",
    "        if (self.loopIter + 1) % 3 == 0:\n",
    "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
    "        self.loopIter = (self.loopIter + 1) % 30\n",
    "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
    "\n",
    "        # 小鸟运动\n",
    "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
    "            self.playerVelY += self.playerAccY\n",
    "        if self.playerFlapped:\n",
    "            self.playerFlapped = False\n",
    "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
    "        if self.playery < 0:\n",
    "            self.playery = 0\n",
    "\n",
    "        # 管道的移动\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            uPipe['x'] += self.pipeVelX\n",
    "            lPipe['x'] += self.pipeVelX\n",
    "\n",
    "        # 当管道快到左侧边缘的时候，产生新的管道\n",
    "        if 0 < self.upperPipes[0]['x'] < 5:\n",
    "            newPipe = getRandomPipe()\n",
    "            self.upperPipes.append(newPipe[0])\n",
    "            self.lowerPipes.append(newPipe[1])\n",
    "\n",
    "        # 当第一个管道移出屏幕的时候，就把它删除\n",
    "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
    "            self.upperPipes.pop(0)\n",
    "            self.lowerPipes.pop(0)\n",
    "\n",
    "        # 检查碰撞\n",
    "        isCrash= checkCrash({'x': self.playerx, 'y': self.playery,\n",
    "                             'index': self.playerIndex},\n",
    "                            self.upperPipes, self.lowerPipes)\n",
    "        # 如果有碰撞发生，则游戏结束，terminal＝True\n",
    "        if isCrash:\n",
    "            #SOUNDS['hit'].play()\n",
    "            #SOUNDS['die'].play()\n",
    "            terminal = True\n",
    "            self.__init__()\n",
    "            reward = -1\n",
    "\n",
    "        # 将所有角色都根据每个角色的坐标画到屏幕上\n",
    "        SCREEN.blit(IMAGES['background'], (0,0))\n",
    "\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            SCREEN.blit(IMAGES['pipe'][0], (uPipe['x'], uPipe['y']))\n",
    "            SCREEN.blit(IMAGES['pipe'][1], (lPipe['x'], lPipe['y']))\n",
    "\n",
    "        SCREEN.blit(IMAGES['base'], (self.basex, BASEY))\n",
    "        \n",
    "        # print score so player overlaps the score\n",
    "        # showScore(self.score)\n",
    "        SCREEN.blit(IMAGES['player'][self.playerIndex],\n",
    "                    (self.playerx, self.playery))\n",
    "\n",
    "        # 将当前的游戏屏幕生成一个二维画面返回\n",
    "        image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "        pygame.display.update()\n",
    "        FPSCLOCK.tick(FPS)\n",
    "        #print self.upperPipes[0]['y'] + PIPE_HEIGHT - int(BASEY * 0.2)\n",
    "        # 该函数的输出有三个变量：游戏当前帧的游戏画面，当前获得的游戏得分，游戏是否已经结束\n",
    "        return image_data, reward, terminal\n",
    "    \n",
    "\n",
    "def getRandomPipe():\n",
    "    #随机生成管道的函数\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # 两个管道之间的竖直间隔从下列数中直接取\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs)-1)\n",
    "    gapY = gapYs[index]\n",
    "\n",
    "    #设定新生成管道的位置\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 10\n",
    "\n",
    "    # 返回管道的坐标\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
    "    ]\n",
    "\n",
    "\n",
    "def showScore(score):\n",
    "    # 在屏幕上直接展示成绩的函数\n",
    "    \"\"\"displays score in center of screen\"\"\"\n",
    "    scoreDigits = [int(x) for x in list(str(score))]\n",
    "    totalWidth = 0 # total width of all numbers to be printed\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        totalWidth += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "    Xoffset = (SCREENWIDTH - totalWidth) / 2\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        SCREEN.blit(IMAGES['numbers'][digit], (Xoffset, SCREENHEIGHT * 0.1))\n",
    "        Xoffset += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "\n",
    "def checkCrash(player, upperPipes, lowerPipes):\n",
    "    # 检测碰撞的函数，基本思路为：将每一个物体都看作是一个矩形区域，然后检查两个矩形区域是否有碰撞\n",
    "    # 检查碰撞是细到每个对象的图像蒙板级别，而不单纯是看矩形之间的碰撞\n",
    "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
    "    pi = player['index']\n",
    "    player['w'] = IMAGES['player'][0].get_width()\n",
    "    player['h'] = IMAGES['player'][0].get_height()\n",
    "\n",
    "    # 检查小鸟是否碰撞到了地面\n",
    "    if player['y'] + player['h'] >= BASEY - 1:\n",
    "        return True\n",
    "    else:\n",
    "        # 检查小鸟是否与管道碰撞\n",
    "        playerRect = pygame.Rect(player['x'], player['y'],\n",
    "                      player['w'], player['h'])\n",
    "\n",
    "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
    "            # 上下管道矩形\n",
    "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "\n",
    "            # 获得每个元素的蒙板\n",
    "            pHitMask = HITMASKS['player'][pi]\n",
    "            uHitmask = HITMASKS['pipe'][0]\n",
    "            lHitmask = HITMASKS['pipe'][1]\n",
    "\n",
    "            # 检查是否与上下管道相撞\n",
    "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
    "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
    "\n",
    "            if uCollide or lCollide:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
    "    \"\"\"在像素级别检查两个物体是否发生碰撞\"\"\"\n",
    "    rect = rect1.clip(rect2)\n",
    "\n",
    "    if rect.width == 0 or rect.height == 0:\n",
    "        return False\n",
    "\n",
    "    # 确定矩形框，并针对矩形框中的每个像素进行循环，查看两个对象是否碰撞\n",
    "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
    "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
    "\n",
    "    for x in range(rect.width):\n",
    "        for y in range(rect.height):\n",
    "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 对游戏做小测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124f3fd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# 新建一个游戏\n",
    "game = GameState()\n",
    "\n",
    "fig = plt.figure()\n",
    "axe = fig.add_subplot(111)\n",
    "dat = np.zeros((10, 10))\n",
    "img = axe.imshow(dat)\n",
    "\n",
    "# 进行100步循环，并将每一帧的画面打印出来\n",
    "for i in range(100):\n",
    "    clear_output(wait = True)\n",
    "    image_data, reward, terminal = game.frame_step([0,1])\n",
    "    \n",
    "    image = np.transpose(image_data, (1, 0, 2))\n",
    "    img.set_data(image)\n",
    "    img.autoscale()\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、训练神经网络玩游戏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 定义网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#  导入必需的包\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2 #需要安装OpenCV的包\n",
    "import sys\n",
    "sys.path.append(\"game/\")\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "# 定义一系列常数，其中，epsilon为每周期随机输出一个动作的概率\n",
    "GAME = 'bird' # 游戏名称\n",
    "ACTIONS = 2 # 有效输出动作的个数\n",
    "GAMMA = 0.99 # 强化学习中未来的衰减率\n",
    "OBSERVE = 10000. # 训练之前的时间步，需要先观察10000帧\n",
    "EXPLORE = 3000000. # 退火所需的时间步，所谓的退火就是指随机选择率epsilon逐渐变小\n",
    "FINAL_EPSILON = 0.0001 # epsilon的最终值\n",
    "INITIAL_EPSILON = 0.1 # epsilon的初始值\n",
    "REPLAY_MEMORY = 50000 # 最多记忆多少帧训练数据\n",
    "BATCH = 32 # 每一个批次的数据记录条数\n",
    "FRAME_PER_ACTION = 1 # 每间隔多少时间完成一次有效动作的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 创建一个多层CNN网络，该网络接收的输入为4帧画面，输出为每个可能动作对应的Q函数值\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 第一层卷积，从4通道到32通道，窗口大小8，跳跃间隔4，填空白2\n",
    "        self.conv1 = nn.Conv2d(4, 32, 8, 4, padding = 2)\n",
    "        # Pooling层，窗口2*2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # 第二层卷积，从32通道到64通道，窗口大小4，跳跃间隔2，填空白1\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2, padding = 1)\n",
    "        # 第二个Pooling层，窗口2＊2，空白1\n",
    "        #self.pool2 = nn.MaxPool2d(2, 2, padding = 1)\n",
    "        # 第三层卷积层，输入输出通道都是64，填空白为1\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1, padding = 1)\n",
    "        \n",
    "        # 最后有两层全链接层\n",
    "        self.fc_sz = 1600\n",
    "        self.fc1 = nn.Linear(self.fc_sz, 256)\n",
    "        self.fc2 = nn.Linear(256, ACTIONS)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入为一个batch的数据，每一个为前后相连的4张图像，每个图像为80*80的大小\n",
    "        # x的尺寸为：batch_size, 4, 80, 80\n",
    "        x = self.conv1(x)\n",
    "        # x的尺寸为：batch_size, 32, 20, 20\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        # x的尺寸为：batch_size, 32, 10, 10\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # x的尺寸为：batch_size, 64, 5, 5\n",
    "        #x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # x的尺寸为：batch_size, 64, 5, 5\n",
    "        #x = self.pool2(x)\n",
    "        # 将x设为1600维的向量, batch_size, 1600\n",
    "        x = x.view(-1, self.fc_sz)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        readout = self.fc2(x)\n",
    "        return readout, x\n",
    "    def init(self):\n",
    "        # 初始化所有的网络权重\n",
    "        self.conv1.weight.data =  torch.abs(0.01 * torch.randn(self.conv1.weight.size()))\n",
    "        self.conv2.weight.data =  torch.abs(0.01 * torch.randn(self.conv2.weight.size()))\n",
    "        self.conv3.weight.data =  torch.abs(0.01 * torch.randn(self.conv3.weight.size()))\n",
    "        self.fc1.weight.data = torch.abs(0.01 * torch.randn(self.fc1.weight.size()))\n",
    "        self.fc2.weight.data = torch.abs(0.01 * torch.randn(self.fc2.weight.size()))\n",
    "        self.conv1.bias.data = torch.ones(self.conv1.bias.size()) * 0.01\n",
    "        self.conv2.bias.data = torch.ones(self.conv2.bias.size()) * 0.01\n",
    "        self.conv3.bias.data = torch.ones(self.conv3.bias.size()) * 0.01\n",
    "        self.fc1.bias.data = torch.ones(self.fc1.bias.size()) * 0.01\n",
    "        self.fc2.bias.data = torch.ones(self.fc2.bias.size()) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 开始在内存／GPU上定义一个网络\n",
    "use_cuda = torch.cuda.is_available() #检测本台机器中是否有GPU\n",
    "\n",
    "# 创建一个神经网络\n",
    "net = Net()\n",
    "# 初始化网络权重。之所以自定义初始化过程是为了增加神经网络权重的多样性\n",
    "net.init()\n",
    "# 如果有GPU，就把神经网络全部搬到GPU内存中做运算\n",
    "net = net.cuda() if use_cuda else net\n",
    "\n",
    "# 定义损失函数为MSE\n",
    "criterion = nn.MSELoss().cuda() if use_cuda else nn.MSELoss()\n",
    "# 定义优化器，并设置初始学习率维10^-6\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-6 )\n",
    "\n",
    "# 开启一个游戏进程，开始与游戏引擎通话\n",
    "game_state = GameState()\n",
    "\n",
    "# 学习样本的存储区域deque是一个类似于list的存储容器\n",
    "D = deque()\n",
    "\n",
    "# 状态打印log记录位置\n",
    "#a_file = open(\"logs_\" + GAME + \"/readout.txt\", 'w')\n",
    "#h_file = open(\"logs_\" + GAME + \"/hidden.txt\", 'w')\n",
    "\n",
    "# 将游戏设置为初始状态，并获得一个80*80的游戏湖面\n",
    "do_nothing = np.zeros(ACTIONS)\n",
    "do_nothing[0] = 1\n",
    "x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "\n",
    "# 将初始的游戏画面叠加成4张作为神经网络的初始输入状态s_t\n",
    "s_t = np.stack((x_t, x_t, x_t, x_t), axis=0)\n",
    "\n",
    "# 设置初始的epsilon（采取随机行动的概率），并准备训练\n",
    "epsilon = INITIAL_EPSILON\n",
    "t = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 边做边学的核心算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该算法分为三个阶段：\n",
    "\n",
    "1、按照Epsilon贪婪算法采取一次行动；\n",
    "2、将选择好的行动输入给游戏引擎，得到下一帧的状态，并生成本帧的训练数据\n",
    "3、开始训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间步 1000/ 状态 observe/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.789328e+04/ 轮得分 1.38\n",
      "时间步 2000/ 状态 observe/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.960289e+04/ 轮得分 1.43\n",
      "时间步 3000/ 状态 observe/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.579410e+04/ 轮得分 1.38\n",
      "时间步 4000/ 状态 observe/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.554549e+04/ 轮得分 1.37\n",
      "时间步 5000/ 状态 observe/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.300679e+04/ 轮得分 1.35\n",
      "时间步 6000/ 状态 observe/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.880158e+04/ 轮得分 1.38\n",
      "时间步 7000/ 状态 observe/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.622109e+04/ 轮得分 1.34\n",
      "时间步 8000/ 状态 observe/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.716134e+04/ 轮得分 1.33\n",
      "时间步 9000/ 状态 observe/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 6.387247e+04/ 轮得分 1.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jake/anaconda/envs/learning_pytorch/lib/python3.6/site-packages/torch/serialization.py:147: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间步 10000/ 状态 observe/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.724834e+04/ 轮得分 1.29\n",
      "时间步 11000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.786650e+04/ 轮得分 1.35\n",
      "损失函数： Variable containing:\n",
      " 1.0755e+08\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 12000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.243400e+04/ 轮得分 1.46\n",
      "损失函数： Variable containing:\n",
      " 6.7522e+07\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 13000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.779126e+04/ 轮得分 1.56\n",
      "损失函数： Variable containing:\n",
      " 3.6268e+07\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 14000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.144210e+04/ 轮得分 1.65\n",
      "损失函数： Variable containing:\n",
      " 1.9996e+07\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 15000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.603308e+04/ 轮得分 1.74\n",
      "损失函数： Variable containing:\n",
      " 1.3511e+07\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 16000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.198958e+04/ 轮得分 1.82\n",
      "损失函数： Variable containing:\n",
      " 1.3862e+06\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 17000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.833563e+04/ 轮得分 1.89\n",
      "损失函数： Variable containing:\n",
      "1.00000e+05 *\n",
      "  7.1176\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 18000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.518621e+04/ 轮得分 1.96\n",
      "损失函数： Variable containing:\n",
      "1.00000e+05 *\n",
      "  5.4290\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 19000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.225043e+04/ 轮得分 2.02\n",
      "损失函数： Variable containing:\n",
      "1.00000e+06 *\n",
      "  2.9913\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 20000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 9.995000e+03/ 轮得分 2.08\n",
      "损失函数： Variable containing:\n",
      "1.00000e+06 *\n",
      "  2.1189\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 21000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 8.255901e+03/ 轮得分 2.14\n",
      "损失函数： Variable containing:\n",
      "1.00000e+06 *\n",
      "  6.6139\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 22000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 6.854616e+03/ 轮得分 2.19\n",
      "损失函数： Variable containing:\n",
      " 77207.2578\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 23000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 5.447451e+03/ 轮得分 2.24\n",
      "损失函数： Variable containing:\n",
      " 86194.8594\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 24000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.343163e+03/ 轮得分 2.29\n",
      "损失函数： Variable containing:\n",
      "1.00000e+05 *\n",
      "  7.8751\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 25000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.590922e+03/ 轮得分 2.33\n",
      "损失函数： Variable containing:\n",
      "1.00000e+05 *\n",
      "  5.0939\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 26000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.895208e+03/ 轮得分 2.37\n",
      "损失函数： Variable containing:\n",
      " 10275.3477\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 27000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.250842e+03/ 轮得分 2.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e+05 *\n",
      "  1.1388\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 28000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.765346e+03/ 轮得分 2.45\n",
      "损失函数： Variable containing:\n",
      "1.00000e+05 *\n",
      "  2.4153\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 29000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.391781e+03/ 轮得分 2.49\n",
      "损失函数： Variable containing:\n",
      " 3148.0225\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 30000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.048499e+03/ 轮得分 2.52\n",
      "损失函数： Variable containing:\n",
      " 46311.5820\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 31000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 8.054719e+02/ 轮得分 2.55\n",
      "损失函数： Variable containing:\n",
      " 747.2329\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 32000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 6.096006e+02/ 轮得分 2.59\n",
      "损失函数： Variable containing:\n",
      " 657.3144\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 33000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.688770e+02/ 轮得分 2.62\n",
      "损失函数： Variable containing:\n",
      " 8019.3076\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 34000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.460710e+02/ 轮得分 2.64\n",
      "损失函数： Variable containing:\n",
      " 7004.6172\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 35000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.419233e+02/ 轮得分 2.67\n",
      "损失函数： Variable containing:\n",
      " 71.2092\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 36000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.821465e+02/ 轮得分 2.70\n",
      "损失函数： Variable containing:\n",
      " 47.2320\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 37000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.264676e+02/ 轮得分 2.70\n",
      "损失函数： Variable containing:\n",
      " 498.9266\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 38000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 8.441042e+01/ 轮得分 2.71\n",
      "损失函数： Variable containing:\n",
      " 494.3927\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 39000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 6.708031e+01/ 轮得分 2.75\n",
      "损失函数： Variable containing:\n",
      " 261.8817\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 40000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 5.352489e+01/ 轮得分 2.80\n",
      "损失函数： Variable containing:\n",
      " 202.3986\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 41000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.568374e+01/ 轮得分 2.85\n",
      "损失函数： Variable containing:\n",
      " 95.0952\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 42000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.395356e+01/ 轮得分 2.90\n",
      "损失函数： Variable containing:\n",
      " 5.7271\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 43000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.084338e+01/ 轮得分 2.95\n",
      "损失函数： Variable containing:\n",
      " 4.2941\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 44000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.098578e+01/ 轮得分 3.01\n",
      "损失函数： Variable containing:\n",
      " 7.2791\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 45000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.971716e+01/ 轮得分 3.06\n",
      "损失函数： Variable containing:\n",
      " 20.4865\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 46000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.976136e+01/ 轮得分 3.11\n",
      "损失函数： Variable containing:\n",
      " 53.0495\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 47000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.814162e+01/ 轮得分 3.16\n",
      "损失函数： Variable containing:\n",
      " 3.1703\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 48000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.610117e+01/ 轮得分 3.22\n",
      "损失函数： Variable containing:\n",
      " 52.9959\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 49000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.462875e+01/ 轮得分 3.26\n",
      "损失函数： Variable containing:\n",
      " 12.3789\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 50000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.891937e+01/ 轮得分 3.31\n",
      "损失函数： Variable containing:\n",
      " 47.3241\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 51000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.962435e+01/ 轮得分 3.37\n",
      "损失函数： Variable containing:\n",
      " 4.7586\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 52000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 5.353829e+01/ 轮得分 3.42\n",
      "损失函数： Variable containing:\n",
      " 16.6850\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 53000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 6.674519e+01/ 轮得分 3.47\n",
      "损失函数： Variable containing:\n",
      " 3.0565\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 54000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 6.820251e+01/ 轮得分 3.52\n",
      "损失函数： Variable containing:\n",
      " 15.8950\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 55000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 7.362189e+01/ 轮得分 3.58\n",
      "损失函数： Variable containing:\n",
      " 22.8050\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 56000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 8.366651e+01/ 轮得分 3.64\n",
      "损失函数： Variable containing:\n",
      " 348.9384\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 57000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 8.478074e+01/ 轮得分 3.70\n",
      "损失函数： Variable containing:\n",
      " 167.5469\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 58000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.023609e+02/ 轮得分 3.75\n",
      "损失函数： Variable containing:\n",
      " 240.6999\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 59000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.046865e+02/ 轮得分 3.80\n",
      "损失函数： Variable containing:\n",
      " 19.3865\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 60000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.092857e+02/ 轮得分 3.85\n",
      "损失函数： Variable containing:\n",
      " 249.7832\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 61000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 -1/ Q_MAX 8.944297e+01/ 轮得分 3.85\n",
      "损失函数： Variable containing:\n",
      " 28.0654\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 62000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.255294e+02/ 轮得分 3.84\n",
      "损失函数： Variable containing:\n",
      " 50.3773\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 63000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.107612e+02/ 轮得分 3.83\n",
      "损失函数： Variable containing:\n",
      " 141.6721\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 64000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 8.338038e+01/ 轮得分 3.83\n",
      "损失函数： Variable containing:\n",
      " 66.0139\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 65000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 7.284401e+01/ 轮得分 3.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      " 29.7879\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 66000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 6.003176e+01/ 轮得分 3.84\n",
      "损失函数： Variable containing:\n",
      " 16.9496\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 67000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.286507e+01/ 轮得分 3.84\n",
      "损失函数： Variable containing:\n",
      " 21.4764\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 68000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.743182e+01/ 轮得分 3.84\n",
      "损失函数： Variable containing:\n",
      " 2.0106\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 69000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.269907e+00/ 轮得分 3.84\n",
      "损失函数： Variable containing:\n",
      " 2.1155\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 70000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.312591e+01/ 轮得分 3.85\n",
      "损失函数： Variable containing:\n",
      " 5.7124\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 71000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX -4.687382e+00/ 轮得分 3.85\n",
      "损失函数： Variable containing:\n",
      " 1.4033\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 72000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.221955e+01/ 轮得分 3.86\n",
      "损失函数： Variable containing:\n",
      " 1.1319\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 73000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.100258e+01/ 轮得分 3.86\n",
      "损失函数： Variable containing:\n",
      " 0.8453\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 74000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.053807e+00/ 轮得分 3.86\n",
      "损失函数： Variable containing:\n",
      " 0.7426\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 75000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.739706e+00/ 轮得分 3.86\n",
      "损失函数： Variable containing:\n",
      " 0.2020\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 76000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 6.390764e+00/ 轮得分 3.86\n",
      "损失函数： Variable containing:\n",
      " 0.1148\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 77000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.198420e+00/ 轮得分 3.87\n",
      "损失函数： Variable containing:\n",
      " 0.1705\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 78000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 5.760870e+00/ 轮得分 3.86\n",
      "损失函数： Variable containing:\n",
      " 0.9645\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 79000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.814322e+00/ 轮得分 3.85\n",
      "损失函数： Variable containing:\n",
      " 0.4021\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 80000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.189880e+00/ 轮得分 3.85\n",
      "损失函数： Variable containing:\n",
      " 0.2801\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 81000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.399968e+00/ 轮得分 3.86\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.2248\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 82000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 5.144660e+00/ 轮得分 3.86\n",
      "损失函数： Variable containing:\n",
      " 0.1148\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 83000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.949995e+00/ 轮得分 3.87\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.5818\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 84000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.363310e+00/ 轮得分 3.88\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.5495\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 85000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 1/ Q_MAX 7.276160e-01/ 轮得分 3.89\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.1538\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 86000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.814284e+00/ 轮得分 3.92\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.3967\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 87000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.604836e+00/ 轮得分 3.95\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.6371\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 88000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX -1.035060e+00/ 轮得分 3.98\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.4050\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 89000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.793057e+00/ 轮得分 4.01\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.2335\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 90000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.529938e+00/ 轮得分 4.03\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.2902\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 91000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 6.761954e-01/ 轮得分 4.06\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.5530\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 92000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.656382e+00/ 轮得分 4.08\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.3006\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 93000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.745706e-01/ 轮得分 4.09\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0889\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 94000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.506513e+00/ 轮得分 4.11\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  6.2967\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 95000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.688650e+00/ 轮得分 4.12\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.4175\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 96000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.699875e+00/ 轮得分 4.13\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.6484\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 97000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.663979e+00/ 轮得分 4.14\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  6.5728\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 98000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.096987e+00/ 轮得分 4.15\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.3432\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 99000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.527166e+00/ 轮得分 4.16\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.4896\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 100000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX -9.469342e-02/ 轮得分 4.18\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.4667\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 101000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.791050e+00/ 轮得分 4.19\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.7743\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 102000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX -5.529668e-01/ 轮得分 4.20\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  2.1421\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 103000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.838141e+00/ 轮得分 4.21\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.1016\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 104000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.039119e+00/ 轮得分 4.22\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.5993\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 105000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.899951e+00/ 轮得分 4.23\n",
      "损失函数： Variable containing:\n",
      " 0.1032\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 106000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.402649e-01/ 轮得分 4.24\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.5755\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 107000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX -2.879517e-01/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0033\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 108000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX -1.507035e-01/ 轮得分 4.26\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  6.2774\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 109000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.459848e+00/ 轮得分 4.26\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.5312\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 110000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.202499e+00/ 轮得分 4.27\n",
      "损失函数： Variable containing:\n",
      " 0.1790\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 111000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 6.710559e-01/ 轮得分 4.27\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.0085\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 112000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 8.362563e-01/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.3787\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 113000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX -6.823626e-01/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.9275\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 114000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 9.727315e-01/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  3.0155\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 115000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.874149e-01/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  9.7487\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 116000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.660152e+00/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  6.9248\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 117000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.841635e+00/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.6248\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间步 118000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.970870e+00/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.0209\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 119000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.609194e+00/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.2891\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 120000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 6.224102e-01/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      " 0.1128\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 121000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 7.627485e-01/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  2.6741\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 122000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.028872e+00/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  2.0903\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 123000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.102388e+00/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.0795\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 124000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 5.943851e-01/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.2017\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 125000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX -2.865220e-02/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.2318\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 126000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.694938e+00/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  9.6686\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 127000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.512997e+00/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.0560\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 128000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.686864e+00/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.0722\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 129000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.340488e+00/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      " 0.2058\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 130000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.302524e+00/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.0910\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 131000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.246070e+00/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.2069\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 132000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.984652e+00/ 轮得分 4.44\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.4787\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 133000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.506449e+00/ 轮得分 4.46\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.0326\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 134000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX -1.353080e-01/ 轮得分 4.48\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3336\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 135000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.203640e+00/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.9630\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 136000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.495420e-01/ 轮得分 4.48\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7573\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 137000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.666026e+00/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.6442\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 138000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.932020e+00/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.9977\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 139000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.486425e+00/ 轮得分 4.48\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.1729\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 140000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.126147e+00/ 轮得分 4.47\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.3859\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 141000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.532912e+00/ 轮得分 4.47\n",
      "损失函数： Variable containing:\n",
      " 0.7774\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 142000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX -3.039415e-01/ 轮得分 4.47\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  6.7305\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 143000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.256190e+00/ 轮得分 4.45\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.4966\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 144000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.429469e+00/ 轮得分 4.44\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  6.7553\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 145000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.814259e-01/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  6.3115\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 146000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.780546e+00/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.3230\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 147000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.267040e+00/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0259\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 148000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.189147e+00/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      " 0.4879\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 149000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.623715e+00/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.4002\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 150000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 9.679124e-01/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8512\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 151000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.222498e+00/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  8.9909\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 152000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 4.125794e+00/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.4690\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 153000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.204271e+00/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.8734\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 154000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.956479e+00/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      " 0.1933\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 155000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.781631e+00/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.0909\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 156000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.455359e+00/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.3545\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 157000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 3.312284e+00/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  2.6288\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 158000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 2.129047e+00/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  3.6188\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 159000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.681695e+00/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.3118\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 160000/ 状态 explore/ Epsilon 0.10/ 行动 0/ 奖励 0.1/ Q_MAX 1.315078e+00/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.4334\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 161000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX -1.798764e-01/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  6.4195\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 162000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.240698e+00/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      " 0.1593\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 163000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 7.966970e-01/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.0495\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 164000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.686190e+00/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.9332\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 165000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.751555e+00/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.2188\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 166000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.441380e+00/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7108\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 167000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.056310e+00/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.5508\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 168000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.665968e+00/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      " 0.2426\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 169000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.182119e+00/ 轮得分 4.35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  3.6885\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 170000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.567266e+00/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.2598\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 171000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.283438e+00/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.2776\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 172000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.451678e+00/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.4749\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 173000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.256110e+00/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0106\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 174000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.975239e+00/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0172\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 175000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.746051e+00/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.8971\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 176000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.142635e-02/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.1478\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 177000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.974587e+00/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.9876\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 178000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 8.009560e-01/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      " 0.1952\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 179000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 -1/ Q_MAX -1.870465e-01/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.3978\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 180000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.971655e+00/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.7617\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 181000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.352381e+00/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.9596\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 182000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.084183e+00/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.1761\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 183000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.408091e+00/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.6722\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 184000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.448225e+00/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.0182\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 185000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 8.766902e-01/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      " 0.1100\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 186000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.295255e+00/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0178\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 187000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.473433e+00/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.4767\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 188000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.411961e+00/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.6388\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 189000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.076005e+00/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  1.9895\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 190000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.795142e+00/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  9.9868\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 191000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.583624e+00/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.1856\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 192000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.573678e+00/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.8388\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 193000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.498545e+00/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7610\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 194000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.318529e+00/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  3.8906\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 195000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.729977e+00/ 轮得分 4.29\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.3901\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 196000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 6.798547e-01/ 轮得分 4.29\n",
      "损失函数： Variable containing:\n",
      " 0.1049\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 197000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.855759e+00/ 轮得分 4.29\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.3063\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 198000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 8.278618e-01/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  8.4422\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 199000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.321923e+00/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.6840\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 200000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.643428e+00/ 轮得分 4.27\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.4291\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 201000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.640336e+00/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.3660\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 202000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.961269e+00/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.5812\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 203000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.530495e+00/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.4534\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 204000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.990028e+00/ 轮得分 4.29\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0611\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 205000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.034248e+00/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.3028\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 206000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.386049e+00/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.0852\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 207000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.353482e+00/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.5172\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 208000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.873267e+00/ 轮得分 4.29\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.3692\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 209000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.209807e+00/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.3808\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 210000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.165006e+00/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  1.9762\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 211000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.070430e+00/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.5775\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 212000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.201899e+00/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.7763\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 213000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.509526e+00/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.1854\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 214000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 7.894616e-01/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  9.6327\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 215000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.516360e+00/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.0912\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 216000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.844727e+00/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.6351\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 217000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.407630e+00/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  2.5615\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 218000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX -2.406876e-01/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.4841\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 219000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.098187e+00/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.6988\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 220000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.561702e+00/ 轮得分 4.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  8.4875\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 221000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.804514e+00/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0757\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 222000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX -5.387659e-01/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  3.6916\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 223000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.920215e-01/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.9186\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 224000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.198164e+00/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.1270\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 225000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.481937e+00/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.4547\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 226000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.793325e+00/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.1864\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 227000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.672730e+00/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.8917\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 228000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.252547e+00/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.3602\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 229000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.734449e-01/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.4173\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 230000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.854578e+00/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.2961\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 231000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.303569e+00/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.3815\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 232000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.060877e+00/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  8.4852\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 233000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 -1/ Q_MAX -8.878362e-01/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.1982\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 234000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.200967e+00/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.9971\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 235000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.180233e+00/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.8848\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 236000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.097120e+00/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.7016\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 237000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.745209e+00/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.8130\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 238000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 9.131290e-01/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  6.2305\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 239000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.515205e+00/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.1234\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 240000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 9.415168e-01/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.0665\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 241000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 9.557881e-01/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.5361\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 242000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.454288e+00/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  8.2167\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 243000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.182062e+00/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.1685\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 244000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.540051e+00/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      " 0.1714\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 245000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.219791e+00/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.3922\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 246000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.348090e+00/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.1163\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 247000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.922713e+00/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.1971\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 248000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.960016e+00/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.8522\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 249000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.857918e+00/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.2068\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 250000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.088400e+00/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.4292\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 251000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.277697e+00/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.4007\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 252000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.421256e+00/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.0452\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 253000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.264945e+00/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.5638\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 254000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.700505e+00/ 轮得分 4.46\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.2704\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 255000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.143465e+00/ 轮得分 4.47\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.8101\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 256000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.110857e+00/ 轮得分 4.48\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.7556\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 257000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.269984e+00/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  3.9011\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 258000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 9.742790e-01/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.5622\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 259000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.729200e+00/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.6988\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 260000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.030334e+00/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.1675\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 261000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.868226e+00/ 轮得分 4.55\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.8443\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 262000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.365647e-01/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.7249\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 263000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.462058e+00/ 轮得分 4.55\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.3048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 264000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.863134e+00/ 轮得分 4.56\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.4642\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 265000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.450577e+00/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.7150\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 266000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.715205e+00/ 轮得分 4.55\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.6964\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 267000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.304871e+00/ 轮得分 4.55\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.6674\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 268000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.894710e+00/ 轮得分 4.55\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.2015\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 269000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.020446e+00/ 轮得分 4.55\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.9902\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 270000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.119064e+00/ 轮得分 4.55\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.8829\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 271000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.879200e+00/ 轮得分 4.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.3012\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 272000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.344627e+00/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  4.1786\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 273000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.828216e+00/ 轮得分 4.58\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.2313\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 274000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.771783e+00/ 轮得分 4.59\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.2580\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 275000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.837083e+00/ 轮得分 4.60\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.8489\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 276000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX -4.286924e-01/ 轮得分 4.61\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.9201\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 277000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.248976e+00/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.4871\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 278000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.039049e+00/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  8.1660\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 279000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.764374e+00/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.1107\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 280000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.421919e+00/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.3899\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 281000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.741218e+00/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  5.7564\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 282000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.942236e+00/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.9568\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 283000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.002876e+00/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  8.5642\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 284000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.515109e+00/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.4564\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 285000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.200373e+00/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.6638\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 286000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.621712e+00/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  6.1519\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 287000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.267298e+00/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.7373\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 288000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.479605e+00/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.2162\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 289000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.995621e+00/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.1208\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 290000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.032042e+00/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.1307\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 291000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.544744e+00/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.9644\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 292000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.083480e+00/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.6832\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 293000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.180243e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.6572\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 294000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.982644e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.1332\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 295000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.146049e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.3313\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 296000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.720871e+00/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.1633\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 297000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.552896e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.1084\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 298000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.271554e+00/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.0912\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 299000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.412555e+00/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.6290\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 300000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.758686e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.5982\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 301000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.311168e-02/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.4474\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 302000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.174709e+00/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.4171\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 303000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.388140e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.6910\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 304000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.693608e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.7113\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 305000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.054624e+00/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.8575\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 306000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 1/ Q_MAX 2.645115e+00/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.2691\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 307000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.110515e+00/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.4133\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 308000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.369485e+00/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.0004\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 309000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.583212e+00/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.1995\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 310000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.921599e+00/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.5125\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 311000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.616669e+00/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.9629\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 312000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.835088e+00/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.0631\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 313000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 6.080884e-01/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.6185\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 314000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.680009e+00/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.4337\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 315000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.120635e+00/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.0805\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 316000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.635536e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.1823\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 317000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.004099e+00/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.7254\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 318000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.110021e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.7405\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 319000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.645313e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.7056\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 320000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.763641e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      " 0.1696\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 321000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.214249e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      " 0.1419\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 322000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.991456e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.7252\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间步 323000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.017483e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.2682\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 324000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 6.664575e-01/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.9711\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 325000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.137948e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      " 0.1811\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 326000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.355749e+00/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.4008\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 327000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.649241e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.2243\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 328000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.422445e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.5953\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 329000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX -2.500353e-01/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8512\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 330000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.875708e+00/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.7566\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 331000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.016418e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.1390\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 332000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 8.107510e-01/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.3013\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 333000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.762254e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.0001\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 334000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.250811e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      " 0.1113\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 335000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.020406e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.5597\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 336000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.646223e-01/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.8866\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 337000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.713110e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.7501\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 338000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 -1/ Q_MAX 1.091027e+00/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.0582\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 339000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.780879e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      " 0.2224\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 340000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX -9.611492e-02/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      " 0.2072\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 341000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.007819e+00/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.7944\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 342000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.655771e+00/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.2185\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 343000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.186599e+00/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7406\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 344000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.933872e+00/ 轮得分 4.87\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.9342\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 345000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.976781e+00/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.2887\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 346000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.977029e-02/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.3937\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 347000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.409430e+00/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      " 0.1058\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 348000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.679142e+00/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.3140\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 349000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.014437e+00/ 轮得分 4.87\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.8585\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 350000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.711883e+00/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.7724\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 351000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.116838e+00/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.5933\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 352000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 -1/ Q_MAX -8.090329e-02/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      " 0.5685\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 353000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.206585e+00/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.5350\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 354000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.385787e+00/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.7876\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 355000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.129139e+00/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      " 0.1040\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 356000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.990563e+00/ 轮得分 4.83\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.7699\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 357000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.097126e-01/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      " 0.1420\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 358000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.862365e-01/ 轮得分 4.83\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.4270\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 359000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 8.525978e-01/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      " 0.2063\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 360000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.335755e+00/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.3000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 361000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.751003e+00/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.3446\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 362000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.705282e+00/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      " 0.1353\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 363000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.621181e+00/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      " 0.1791\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 364000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.387986e+00/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8415\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 365000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.018159e+00/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.0691\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 366000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 7.106053e-01/ 轮得分 4.83\n",
      "损失函数： Variable containing:\n",
      " 0.1230\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 367000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.184031e+00/ 轮得分 4.83\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.5881\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 368000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.324515e+00/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      " 0.1769\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 369000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.246951e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.3376\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 370000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.517644e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.0142\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 371000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.735716e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      " 0.7663\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 372000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.296570e+00/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.7940\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 373000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.011285e+00/ 轮得分 4.83\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.9547\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 374000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 -1/ Q_MAX 4.707730e+00/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.2204\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间步 375000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.926228e+00/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.2148\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 376000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.160700e+00/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      " 0.1085\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 377000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.443056e+00/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.2688\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 378000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 6.195250e+00/ 轮得分 4.83\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.4946\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 379000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.550964e+00/ 轮得分 4.83\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.7088\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 380000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 -1/ Q_MAX 3.117166e-01/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.5717\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 381000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.720491e+00/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      " 0.1337\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 382000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 6.393108e+00/ 轮得分 4.87\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.1208\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 383000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.822738e+00/ 轮得分 4.87\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.6235\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 384000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.045598e+00/ 轮得分 4.88\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.0044\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 385000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.009601e+00/ 轮得分 4.88\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.1416\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 386000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.630787e+00/ 轮得分 4.89\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8354\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 387000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.957630e+00/ 轮得分 4.90\n",
      "损失函数： Variable containing:\n",
      " 0.1718\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 388000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.828753e+00/ 轮得分 4.89\n",
      "损失函数： Variable containing:\n",
      " 0.1319\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 389000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.715717e+00/ 轮得分 4.89\n",
      "损失函数： Variable containing:\n",
      " 0.1785\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 390000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.513362e+00/ 轮得分 4.89\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.8660\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 391000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.030642e+00/ 轮得分 4.90\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.3809\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 392000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.161024e+00/ 轮得分 4.88\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.1555\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 393000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.597226e+00/ 轮得分 4.88\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.3880\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 394000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.595791e+00/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.5114\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 395000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.846575e+00/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.8669\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 396000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.468159e+00/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.2598\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 397000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.586229e+00/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      " 0.2773\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 398000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.588115e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      " 0.1485\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 399000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX -1.260961e+00/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.4352\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 400000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.474610e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      " 0.1001\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 401000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.566952e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.7049\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 402000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.384618e+00/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.2015\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 403000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.014723e+00/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.6370\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 404000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.040455e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.0246\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 405000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.658350e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.5350\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 406000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 -1/ Q_MAX -4.585293e-01/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.9538\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 407000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 6.589863e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7895\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 408000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.564787e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      " 0.1183\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 409000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.819573e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.2147\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 410000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.099583e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3737\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 411000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.533921e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.3352\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 412000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.416625e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.8814\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 413000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.636318e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.0111\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 414000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.554356e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.5571\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 415000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.465817e+00/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.0630\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 416000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.944914e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.6053\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 417000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.234840e+00/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.9463\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 418000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX -1.016083e+00/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.4741\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 419000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.242329e+00/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7402\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 420000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.584625e+00/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.5441\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 421000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX -2.650415e-01/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.6024\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 422000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.350146e+00/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.2221\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 423000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.566646e+00/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.6892\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 424000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.926906e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.9780\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 425000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.346811e+00/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.6145\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 426000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.424296e+00/ 轮得分 4.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.3964\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 427000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.106454e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.2349\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 428000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.975177e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7062\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 429000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.353487e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.5072\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 430000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.940926e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.6890\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 431000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.625515e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.1996\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 432000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.816441e+00/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.7208\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 433000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.119665e+00/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.0521\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 434000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.792458e+00/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.6521\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 435000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.757099e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.9508\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 436000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.973361e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.9745\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 437000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.532677e+00/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.9284\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 438000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.598365e+00/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.2364\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 439000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 -1/ Q_MAX 3.751616e-01/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.1346\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 440000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.770442e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.0282\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 441000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.470104e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.6146\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 442000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.638797e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.9230\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 443000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.421135e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.1166\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 444000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.870091e+00/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.3646\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 445000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.675200e+00/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.0075\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 446000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.931475e+00/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      " 0.4522\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 447000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.316119e+00/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.4332\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 448000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.435565e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      " 0.1773\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 449000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.809814e+00/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.6686\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 450000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.072508e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.0396\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 451000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.717468e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.6792\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 452000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 1.920022e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3577\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 453000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.598427e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.1949\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 454000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.955926e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      " 0.1652\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 455000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 4.430365e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.2647\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 456000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 3.636609e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.1033\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 457000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 6.263972e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.0817\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 458000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 5.761135e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.8153\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 459000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 6.368053e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.8684\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 460000/ 状态 explore/ Epsilon 0.09/ 行动 0/ 奖励 0.1/ Q_MAX 2.173804e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      " 0.1720\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 461000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 1/ Q_MAX 5.650675e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      " 1.5487\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 462000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 1.703669e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.0720\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 463000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.769477e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7782\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 464000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.849461e+00/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      " 0.4232\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 465000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.094275e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.6832\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 466000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.561437e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.1608\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 467000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.555692e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.7985\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 468000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 2.904265e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.8476\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 469000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.101370e+00/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      " 0.2746\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 470000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.027728e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.5577\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 471000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 1/ Q_MAX 5.652607e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      " 0.1656\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 472000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.486592e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7453\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 473000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.915123e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.1787\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 474000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 2.962107e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.4247\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 475000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.082797e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      " 0.3618\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 476000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.928249e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.1836\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 477000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.806832e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.4953\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 478000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.071238e+00/ 轮得分 4.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.2646\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 479000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.742798e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.3186\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 480000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 1.224088e+00/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      " 0.1092\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 481000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.460814e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.7209\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 482000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.344426e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      " 0.1228\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 483000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.999577e+00/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      " 1.0647\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 484000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.494431e+00/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8250\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 485000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.877029e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      " 0.7181\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 486000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.347825e+00/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.4048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 487000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.732724e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.4057\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 488000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.898251e+00/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.3952\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 489000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.431264e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.1771\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 490000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.848411e+00/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 0.1169\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 491000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.816627e+00/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.1676\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 492000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 1.680751e+00/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.2259\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 493000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.924777e+00/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.9091\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 494000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.316366e+00/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.4348\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 495000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.478532e+00/ 轮得分 4.61\n",
      "损失函数： Variable containing:\n",
      " 0.2505\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 496000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.405294e+00/ 轮得分 4.58\n",
      "损失函数： Variable containing:\n",
      " 0.2335\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 497000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.671231e+00/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      " 0.3891\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 498000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.009400e+00/ 轮得分 4.59\n",
      "损失函数： Variable containing:\n",
      " 0.1528\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 499000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.905421e+00/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      " 0.4432\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 500000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.685874e+00/ 轮得分 4.56\n",
      "损失函数： Variable containing:\n",
      " 0.4296\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 501000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.576466e+00/ 轮得分 4.55\n",
      "损失函数： Variable containing:\n",
      " 0.1534\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 502000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.157187e+00/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.8160\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 503000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.658876e+00/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      " 0.1302\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 504000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.513765e+00/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.1527\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 505000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.140902e+00/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      " 0.1150\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 506000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.265990e+00/ 轮得分 4.52\n",
      "损失函数： Variable containing:\n",
      " 0.1139\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 507000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.804654e-01/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      " 0.1231\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 508000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.599632e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      " 0.1922\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 509000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.888539e+00/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.3957\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 510000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.247372e+00/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      " 0.4938\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 511000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.908566e+00/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.5085\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 512000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.953472e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      " 0.1306\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 513000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 2.308192e+00/ 轮得分 4.52\n",
      "损失函数： Variable containing:\n",
      " 0.1466\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 514000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX -5.692038e-01/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.1042\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 515000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.877236e+00/ 轮得分 4.46\n",
      "损失函数： Variable containing:\n",
      " 0.1932\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 516000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.344380e+00/ 轮得分 4.47\n",
      "损失函数： Variable containing:\n",
      " 0.2239\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 517000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 1/ Q_MAX 1.036328e+00/ 轮得分 4.45\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.3695\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 518000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.206339e+00/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      " 0.2425\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 519000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.189320e+00/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      " 0.1026\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 520000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.286446e+00/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      " 0.2698\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 521000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.290512e+00/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      " 0.1452\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 522000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.669345e+00/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.4838\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 523000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.870522e+00/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      " 0.1308\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 524000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.323731e+00/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      " 0.1645\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 525000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.048570e+00/ 轮得分 4.29\n",
      "损失函数： Variable containing:\n",
      " 0.1282\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 526000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.674719e+00/ 轮得分 4.24\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.4970\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 527000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.456895e+00/ 轮得分 4.22\n",
      "损失函数： Variable containing:\n",
      " 0.1853\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 528000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 2.330010e+00/ 轮得分 4.19\n",
      "损失函数： Variable containing:\n",
      " 0.1182\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 529000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 1.000830e+01/ 轮得分 4.17\n",
      "损失函数： Variable containing:\n",
      " 1.3665\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 530000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.182565e+00/ 轮得分 4.12\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.0628\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 531000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.425512e+00/ 轮得分 4.13\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.9548\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 532000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.047157e+00/ 轮得分 4.11\n",
      "损失函数： Variable containing:\n",
      " 0.2027\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间步 533000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.787586e+00/ 轮得分 4.08\n",
      "损失函数： Variable containing:\n",
      " 0.1419\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 534000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 1.013631e+01/ 轮得分 4.07\n",
      "损失函数： Variable containing:\n",
      " 0.1366\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 535000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 1.066691e+01/ 轮得分 4.05\n",
      "损失函数： Variable containing:\n",
      " 0.1263\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 536000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 1.035022e+01/ 轮得分 4.01\n",
      "损失函数： Variable containing:\n",
      " 0.2073\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 537000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX -7.297578e-01/ 轮得分 3.95\n",
      "损失函数： Variable containing:\n",
      " 0.1369\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 538000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.425323e+00/ 轮得分 3.96\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.3798\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 539000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.165685e+00/ 轮得分 3.94\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.6941\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 540000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.268689e+00/ 轮得分 3.92\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.7062\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 541000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.817330e+00/ 轮得分 3.90\n",
      "损失函数： Variable containing:\n",
      " 0.2163\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 542000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.913501e+00/ 轮得分 3.86\n",
      "损失函数： Variable containing:\n",
      " 0.1716\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 543000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.234545e+00/ 轮得分 3.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.3532\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 544000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.462458e+00/ 轮得分 3.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.8151\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 545000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.827049e+00/ 轮得分 3.86\n",
      "损失函数： Variable containing:\n",
      " 0.1264\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 546000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.643572e+00/ 轮得分 3.83\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.6379\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 547000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.407983e+00/ 轮得分 3.80\n",
      "损失函数： Variable containing:\n",
      " 0.1207\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 548000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.516454e+00/ 轮得分 3.78\n",
      "损失函数： Variable containing:\n",
      " 0.4012\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 549000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.948076e+00/ 轮得分 3.75\n",
      "损失函数： Variable containing:\n",
      " 0.6455\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 550000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.579117e+00/ 轮得分 3.73\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.2150\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 551000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.170980e+00/ 轮得分 3.69\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.5708\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 552000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.871285e+00/ 轮得分 3.65\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.9296\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 553000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.701045e+00/ 轮得分 3.61\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.0452\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 554000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.241511e+00/ 轮得分 3.59\n",
      "损失函数： Variable containing:\n",
      " 0.1598\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 555000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.987335e+00/ 轮得分 3.57\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.6382\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 556000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.806949e+00/ 轮得分 3.57\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.8164\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 557000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.569355e+00/ 轮得分 3.54\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.1574\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 558000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 2.784428e+00/ 轮得分 3.54\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7659\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 559000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.074914e+00/ 轮得分 3.53\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.0591\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 560000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.839176e+00/ 轮得分 3.53\n",
      "损失函数： Variable containing:\n",
      " 0.1019\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 561000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.647146e+00/ 轮得分 3.52\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.0402\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 562000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.360712e+00/ 轮得分 3.51\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.2570\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 563000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.096866e+00/ 轮得分 3.52\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.2401\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 564000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.148500e+00/ 轮得分 3.53\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.1264\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 565000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.683270e+00/ 轮得分 3.50\n",
      "损失函数： Variable containing:\n",
      " 0.1020\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 566000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.845987e+00/ 轮得分 3.50\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.0241\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 567000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.750630e+00/ 轮得分 3.48\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.5010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 568000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.277184e+00/ 轮得分 3.45\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.6935\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 569000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 1.130793e+00/ 轮得分 3.43\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.3882\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 570000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.294701e+00/ 轮得分 3.43\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.4248\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 571000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.973022e+00/ 轮得分 3.43\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.5821\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 572000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.190942e+00/ 轮得分 3.41\n",
      "损失函数： Variable containing:\n",
      " 0.1873\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 573000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.666987e+00/ 轮得分 3.41\n",
      "损失函数： Variable containing:\n",
      " 0.3130\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 574000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.167240e-01/ 轮得分 3.42\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.2819\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 575000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.380058e+00/ 轮得分 3.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.2288\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 576000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.862577e+00/ 轮得分 3.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.3526\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 577000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.414704e+00/ 轮得分 3.39\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.2027\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 578000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.636993e+00/ 轮得分 3.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-03 *\n",
      "  7.9799\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 579000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.390056e+00/ 轮得分 3.39\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.9696\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 580000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.787777e+00/ 轮得分 3.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.0340\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 581000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.136467e+00/ 轮得分 3.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.1321\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 582000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.996251e+00/ 轮得分 3.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.9062\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 583000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 2.444374e+00/ 轮得分 3.43\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.9511\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 584000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.842490e+00/ 轮得分 3.45\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.2775\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 585000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.100554e+00/ 轮得分 3.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.0078\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 586000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.097551e+00/ 轮得分 3.48\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.3785\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 587000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.889677e+00/ 轮得分 3.49\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.8425\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 588000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.924929e+00/ 轮得分 3.49\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.6602\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 589000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.649343e+00/ 轮得分 3.49\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.6582\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 590000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.916698e+00/ 轮得分 3.50\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.7650\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 591000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.736674e+00/ 轮得分 3.52\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.4063\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 592000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.377064e+00/ 轮得分 3.52\n",
      "损失函数： Variable containing:\n",
      " 0.1280\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 593000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.613412e+00/ 轮得分 3.51\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.3740\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 594000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.870162e+00/ 轮得分 3.52\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8740\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 595000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 1.072086e-01/ 轮得分 3.52\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.7441\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 596000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.226488e+00/ 轮得分 3.56\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.0372\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 597000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 -1/ Q_MAX -1.642502e-01/ 轮得分 3.58\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.0391\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 598000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.266744e+00/ 轮得分 3.56\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.5152\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 599000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.507044e+00/ 轮得分 3.59\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8874\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 600000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.924660e+00/ 轮得分 3.58\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.9432\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 601000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.180860e+00/ 轮得分 3.57\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3205\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 602000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.066906e+00/ 轮得分 3.57\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.6313\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 603000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.880224e+00/ 轮得分 3.57\n",
      "损失函数： Variable containing:\n",
      " 0.1101\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 604000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.788334e+00/ 轮得分 3.58\n",
      "损失函数： Variable containing:\n",
      " 0.1370\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 605000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.665245e+00/ 轮得分 3.58\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.6755\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 606000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.991008e+00/ 轮得分 3.58\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.6938\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 607000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.460341e+00/ 轮得分 3.59\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.1172\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 608000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.424664e+00/ 轮得分 3.60\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.1748\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 609000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.775663e+00/ 轮得分 3.60\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.8222\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 610000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX -6.947343e-01/ 轮得分 3.61\n",
      "损失函数： Variable containing:\n",
      " 0.1172\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 611000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 2.323335e+00/ 轮得分 3.63\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.6816\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 612000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.014544e+00/ 轮得分 3.64\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.6022\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 613000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.012498e+00/ 轮得分 3.64\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.3763\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 614000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.658651e+00/ 轮得分 3.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.1743\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 615000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.001602e+00/ 轮得分 3.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.8762\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 616000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.521268e+00/ 轮得分 3.70\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.9983\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 617000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.114262e+00/ 轮得分 3.72\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.8823\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 618000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.390132e+00/ 轮得分 3.72\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.9905\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 619000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.421728e+00/ 轮得分 3.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.9477\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 620000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 1.992456e+00/ 轮得分 3.77\n",
      "损失函数： Variable containing:\n",
      " 0.1850\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 621000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.824628e+00/ 轮得分 3.80\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.2116\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 622000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.005888e+00/ 轮得分 3.81\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.7459\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 623000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.963865e+00/ 轮得分 3.80\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.3894\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 624000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.009742e+00/ 轮得分 3.81\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.4294\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 625000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.688143e+00/ 轮得分 3.84\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.4202\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 626000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.144826e+00/ 轮得分 3.86\n",
      "损失函数： Variable containing:\n",
      " 0.1121\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 627000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.990902e+00/ 轮得分 3.89\n",
      "损失函数： Variable containing:\n",
      " 0.5902\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 628000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.993942e+00/ 轮得分 3.92\n",
      "损失函数： Variable containing:\n",
      " 0.2120\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 629000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.368276e+00/ 轮得分 3.91\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.7971\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 630000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.268814e+00/ 轮得分 3.93\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.1595\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 631000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.097971e+00/ 轮得分 3.92\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.4589\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 632000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.570199e+00/ 轮得分 3.90\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.8620\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 633000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.819525e+00/ 轮得分 3.90\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.7283\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 634000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.285655e+00/ 轮得分 3.91\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.7847\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 635000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.369313e+00/ 轮得分 3.94\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.9786\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 636000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.812763e+00/ 轮得分 3.96\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.0300\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间步 637000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.043895e+00/ 轮得分 4.01\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.4233\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 638000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.104314e+00/ 轮得分 4.02\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.2173\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 639000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.587784e+00/ 轮得分 4.02\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7097\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 640000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 2.778253e+00/ 轮得分 4.02\n",
      "损失函数： Variable containing:\n",
      " 0.1075\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 641000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.351988e+00/ 轮得分 4.08\n",
      "损失函数： Variable containing:\n",
      " 0.1864\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 642000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.617554e+00/ 轮得分 4.10\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.6721\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 643000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.806123e+00/ 轮得分 4.12\n",
      "损失函数： Variable containing:\n",
      " 0.1508\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 644000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.456598e+00/ 轮得分 4.13\n",
      "损失函数： Variable containing:\n",
      " 0.1035\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 645000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.168351e+00/ 轮得分 4.15\n",
      "损失函数： Variable containing:\n",
      " 0.1040\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 646000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.844677e+00/ 轮得分 4.17\n",
      "损失函数： Variable containing:\n",
      " 0.1595\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 647000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.870460e+00/ 轮得分 4.18\n",
      "损失函数： Variable containing:\n",
      " 0.2523\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 648000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.194910e+00/ 轮得分 4.19\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.6042\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 649000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.478555e+00/ 轮得分 4.18\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.0351\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 650000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.421003e+00/ 轮得分 4.19\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.5882\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 651000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.219811e+00/ 轮得分 4.24\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.7680\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 652000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.347187e+00/ 轮得分 4.27\n",
      "损失函数： Variable containing:\n",
      " 0.1380\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 653000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 -1/ Q_MAX 4.802251e-01/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.5122\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 654000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.917054e+00/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.0562\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 655000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.784899e+00/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.8204\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 656000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.483684e+00/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.0909\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 657000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.468825e+00/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.6349\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 658000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.629525e+00/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.0021\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 659000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.414773e+00/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      " 0.1017\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 660000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.541504e+00/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.1435\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 661000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.145018e+00/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.2995\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 662000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.138884e+00/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      " 0.7228\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 663000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.652884e+00/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.7234\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 664000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.333730e+00/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      " 0.2658\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 665000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 1.987038e+00/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.2992\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 666000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.103281e+00/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8871\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 667000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.436697e+00/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.5771\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 668000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.245127e+00/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.1190\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 669000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.139745e+00/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.3860\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 670000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.191355e+00/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      " 0.3493\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 671000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.308913e+00/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.4012\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 672000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.159694e+00/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.2079\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 673000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.876071e+00/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      " 0.2866\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 674000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 2.003785e+00/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.8112\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 675000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.656945e+00/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      " 0.1760\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 676000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.188522e+00/ 轮得分 4.45\n",
      "损失函数： Variable containing:\n",
      " 0.1349\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 677000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.663997e+00/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.0453\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 678000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.328012e+00/ 轮得分 4.44\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.6266\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 679000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.313798e+00/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.1891\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 680000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.840471e+00/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      " 0.1040\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 681000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.982079e+00/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.5401\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 682000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.168094e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.8777\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 683000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.557064e+00/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.4103\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 684000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.325695e+00/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.9358\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 685000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.346534e+00/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.0847\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 686000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.396455e+00/ 轮得分 4.58\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.8550\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 687000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.543160e+00/ 轮得分 4.59\n",
      "损失函数： Variable containing:\n",
      " 0.2533\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 688000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.317168e+00/ 轮得分 4.58\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.7007\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 689000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.366392e+00/ 轮得分 4.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.3263\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 690000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.838066e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      " 0.1483\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 691000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.636801e+00/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      " 0.3572\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 692000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.462094e+00/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.4665\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 693000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.342949e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.6205\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 694000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.966805e+00/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.0685\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 695000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.069536e+00/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      " 0.1308\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 696000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.649932e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.1444\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 697000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.291472e+00/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      " 0.2550\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 698000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.652285e+00/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.6329\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 699000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.509912e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.1688\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 700000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.880991e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      " 0.2966\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 701000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.335852e+00/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.5759\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 702000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.494904e+00/ 轮得分 4.56\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.0395\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 703000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.195168e+00/ 轮得分 4.56\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.5399\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 704000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.908366e+00/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      " 0.1058\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 705000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.357394e+00/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      " 0.1370\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 706000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.870574e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.9443\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 707000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.326526e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.3188\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 708000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.616529e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.7016\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 709000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.402902e+00/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      " 0.3123\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 710000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.624420e+00/ 轮得分 4.55\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.7366\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 711000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.073571e+00/ 轮得分 4.55\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.9025\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 712000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.490974e+00/ 轮得分 4.55\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.2900\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 713000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.785096e+00/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.0640\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 714000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.684696e+00/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.1234\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 715000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.185677e+00/ 轮得分 4.60\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.6456\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 716000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.851161e+00/ 轮得分 4.61\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3818\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 717000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.455946e+00/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.1084\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 718000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.442046e+00/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.9443\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 719000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.846304e+00/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.2851\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 720000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.938630e+00/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.3229\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 721000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.611383e+00/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.0674\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 722000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.007263e+00/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.2109\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 723000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 3.941078e+00/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.1579\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 724000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.326959e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.1533\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 725000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.404627e+00/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.1673\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 726000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.317223e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      " 0.1160\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 727000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.526779e+00/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      " 0.1469\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 728000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.802222e+00/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      " 0.1103\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 729000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 2.995920e+00/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      " 0.1039\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 730000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.668383e+00/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      " 0.2921\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 731000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.423178e+00/ 轮得分 4.91\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.9382\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 732000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.976577e+00/ 轮得分 4.92\n",
      "损失函数： Variable containing:\n",
      " 0.1302\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 733000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.235021e+00/ 轮得分 4.95\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.6923\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 734000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.156199e+00/ 轮得分 4.94\n",
      "损失函数： Variable containing:\n",
      " 0.1488\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 735000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.517673e+00/ 轮得分 4.95\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.9093\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 736000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.778960e+00/ 轮得分 4.93\n",
      "损失函数： Variable containing:\n",
      " 0.1737\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 737000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.121263e+00/ 轮得分 4.91\n",
      "损失函数： Variable containing:\n",
      " 0.1093\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 738000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.898055e+00/ 轮得分 4.90\n",
      "损失函数： Variable containing:\n",
      " 0.1162\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 739000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 4.281465e+00/ 轮得分 4.91\n",
      "损失函数： Variable containing:\n",
      " 0.1543\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 740000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.378765e+00/ 轮得分 4.92\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.3995\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 741000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.836736e+00/ 轮得分 4.93\n",
      "损失函数： Variable containing:\n",
      " 0.1215\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 742000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.711775e+00/ 轮得分 4.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      " 0.1726\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 743000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.479308e+00/ 轮得分 4.92\n",
      "损失函数： Variable containing:\n",
      " 0.1030\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 744000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.208384e+00/ 轮得分 4.92\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.4080\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 745000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 6.623540e+00/ 轮得分 4.94\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.6490\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 746000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.919075e+00/ 轮得分 4.98\n",
      "损失函数： Variable containing:\n",
      " 0.1041\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 747000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.161046e+00/ 轮得分 4.98\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.2420\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 748000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.337006e+00/ 轮得分 4.97\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.4202\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 749000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.089469e+00/ 轮得分 4.97\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.8712\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 750000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.637947e+00/ 轮得分 5.00\n",
      "损失函数： Variable containing:\n",
      " 0.2256\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 751000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.488914e+00/ 轮得分 5.01\n",
      "损失函数： Variable containing:\n",
      " 0.1013\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 752000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.047885e+00/ 轮得分 5.00\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.3467\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 753000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.842008e+00/ 轮得分 4.97\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3984\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 754000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 7.115561e+00/ 轮得分 4.96\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.7584\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 755000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.985937e+00/ 轮得分 5.02\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.9160\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 756000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 9.740478e+00/ 轮得分 5.04\n",
      "损失函数： Variable containing:\n",
      " 0.5457\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 757000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 5.576454e+00/ 轮得分 5.04\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.8269\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 758000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.304893e+00/ 轮得分 5.05\n",
      "损失函数： Variable containing:\n",
      " 0.1492\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 759000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 8.078030e+00/ 轮得分 5.04\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.4559\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 760000/ 状态 explore/ Epsilon 0.08/ 行动 0/ 奖励 0.1/ Q_MAX 1.408140e+00/ 轮得分 5.05\n",
      "损失函数： Variable containing:\n",
      " 0.3317\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 761000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.613992e+00/ 轮得分 5.04\n",
      "损失函数： Variable containing:\n",
      " 0.1459\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 762000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.470189e+00/ 轮得分 5.04\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.4677\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 763000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 4.951382e-01/ 轮得分 5.01\n",
      "损失函数： Variable containing:\n",
      " 0.1078\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 764000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.085454e+00/ 轮得分 5.01\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.8084\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 765000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.001978e+01/ 轮得分 4.97\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.0650\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 766000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 5.637676e+00/ 轮得分 4.95\n",
      "损失函数： Variable containing:\n",
      " 0.1524\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 767000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 4.068853e+00/ 轮得分 4.94\n",
      "损失函数： Variable containing:\n",
      " 0.5119\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 768000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.073318e+01/ 轮得分 4.97\n",
      "损失函数： Variable containing:\n",
      " 0.1684\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 769000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 6.629106e+00/ 轮得分 4.96\n",
      "损失函数： Variable containing:\n",
      " 0.1860\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 770000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.450769e-01/ 轮得分 4.95\n",
      "损失函数： Variable containing:\n",
      " 0.3553\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 771000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.644117e+00/ 轮得分 4.91\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.3048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 772000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 2.927793e+00/ 轮得分 4.92\n",
      "损失函数： Variable containing:\n",
      " 0.3028\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 773000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 5.171505e+00/ 轮得分 4.90\n",
      "损失函数： Variable containing:\n",
      " 0.2634\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 774000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.899482e+00/ 轮得分 4.89\n",
      "损失函数： Variable containing:\n",
      " 0.1019\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 775000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.004045e+01/ 轮得分 4.87\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8861\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 776000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 6.609706e+00/ 轮得分 4.91\n",
      "损失函数： Variable containing:\n",
      " 0.1641\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 777000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.794863e+00/ 轮得分 4.94\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.1786\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 778000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.895391e+00/ 轮得分 4.90\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.9808\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 779000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.003364e+01/ 轮得分 4.89\n",
      "损失函数： Variable containing:\n",
      " 0.3049\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 780000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 6.989982e+00/ 轮得分 4.88\n",
      "损失函数： Variable containing:\n",
      " 1.3554\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 781000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.587468e+00/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      " 1.0966\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 782000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.624921e+00/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      " 0.2076\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 783000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 6.102924e+00/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8323\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 784000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.919415e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      " 0.1082\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 785000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 -1/ Q_MAX 1.088471e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7465\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 786000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.082530e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.1704\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 787000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 5.657656e+00/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.2670\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 788000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.650827e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.2565\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 789000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.903725e+00/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.9106\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 790000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.703662e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.1255\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 791000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.421048e+00/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.0504\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 792000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.400106e+00/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.1437\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 793000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.487808e+00/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.7005\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 794000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.085611e+00/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.3444\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 795000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.191952e+00/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.2084\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间步 796000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.834474e+00/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.5583\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 797000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.042005e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.9953\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 798000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.262509e+00/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 0.1756\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 799000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.643373e+00/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.1818\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 800000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.848341e+00/ 轮得分 4.60\n",
      "损失函数： Variable containing:\n",
      " 0.1068\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 801000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.047868e+01/ 轮得分 4.61\n",
      "损失函数： Variable containing:\n",
      " 0.2598\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 802000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.005512e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.8418\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 803000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 6.475902e+00/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.5293\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 804000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.588854e+00/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.1414\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 805000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.184536e+00/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.2100\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 806000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 5.782012e+00/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.2424\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 807000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.885406e+00/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7056\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 808000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.666492e+00/ 轮得分 4.60\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.9019\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 809000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.389928e+00/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.1736\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 810000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.002621e+01/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.4643\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 811000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.617343e+00/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.2896\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 812000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 6.163778e+00/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.4014\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 813000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 5.876341e+00/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.8832\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 814000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.945156e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.2693\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 815000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.932110e+00/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.1395\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 816000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.011439e+01/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      " 0.2211\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 817000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 1/ Q_MAX 8.323631e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.1144\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 818000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.702004e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.2877\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 819000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.935163e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.1484\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 820000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.093647e+00/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.6686\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 821000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.341781e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.1670\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 822000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.525272e+00/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      " 0.1954\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 823000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.074293e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.1051\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 824000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 5.682154e+00/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      " 0.1166\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 825000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 5.201239e+00/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      " 0.8003\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 826000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.919317e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      " 0.2428\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 827000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.048053e+00/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      " 0.2896\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 828000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.209698e+00/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.7512\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 829000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.355278e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      " 0.1412\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 830000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.513616e+00/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      " 0.3299\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 831000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.134548e+01/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      " 0.2761\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 832000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.270872e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      " 0.2836\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 833000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.128106e+01/ 轮得分 4.83\n",
      "损失函数： Variable containing:\n",
      " 0.2886\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 834000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 -1/ Q_MAX 1.563215e+00/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      " 0.2196\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 835000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.198102e+01/ 轮得分 4.88\n",
      "损失函数： Variable containing:\n",
      " 0.1266\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 836000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.108082e+01/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.5835\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 837000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.394132e+00/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      " 0.4171\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 838000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.199724e+01/ 轮得分 4.90\n",
      "损失函数： Variable containing:\n",
      " 0.2243\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 839000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.574192e+00/ 轮得分 4.89\n",
      "损失函数： Variable containing:\n",
      " 0.1099\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 840000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.251958e+01/ 轮得分 4.88\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.0526\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 841000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.190070e+00/ 轮得分 4.91\n",
      "损失函数： Variable containing:\n",
      " 0.1747\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 842000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.150289e+01/ 轮得分 4.89\n",
      "损失函数： Variable containing:\n",
      " 0.1592\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 843000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.261600e+01/ 轮得分 4.89\n",
      "损失函数： Variable containing:\n",
      " 0.7033\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 844000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.922487e+00/ 轮得分 4.91\n",
      "损失函数： Variable containing:\n",
      " 0.1944\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 845000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.553853e+00/ 轮得分 4.95\n",
      "损失函数： Variable containing:\n",
      " 0.2179\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 846000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.207206e+01/ 轮得分 4.99\n",
      "损失函数： Variable containing:\n",
      " 0.1937\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 847000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.610616e+00/ 轮得分 4.99\n",
      "损失函数： Variable containing:\n",
      " 0.1385\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 848000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 -1/ Q_MAX 1.151743e+01/ 轮得分 4.96\n",
      "损失函数： Variable containing:\n",
      " 0.5582\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 849000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.175741e+01/ 轮得分 4.95\n",
      "损失函数： Variable containing:\n",
      " 0.1662\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 850000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.283271e+01/ 轮得分 4.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      " 0.8895\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 851000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.851604e+00/ 轮得分 4.93\n",
      "损失函数： Variable containing:\n",
      " 0.1004\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 852000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 6.707787e+00/ 轮得分 4.97\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.5796\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 853000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.726897e+00/ 轮得分 4.97\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.9174\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 854000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.076023e+01/ 轮得分 4.98\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.0239\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 855000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.174711e+01/ 轮得分 5.01\n",
      "损失函数： Variable containing:\n",
      " 0.1536\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 856000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.193016e+00/ 轮得分 5.04\n",
      "损失函数： Variable containing:\n",
      " 0.1497\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 857000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.850941e+00/ 轮得分 5.07\n",
      "损失函数： Variable containing:\n",
      " 0.3168\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 858000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.587150e+00/ 轮得分 5.03\n",
      "损失函数： Variable containing:\n",
      " 0.1142\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 859000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.187809e+01/ 轮得分 5.03\n",
      "损失函数： Variable containing:\n",
      " 0.2661\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 860000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.064276e+01/ 轮得分 5.02\n",
      "损失函数： Variable containing:\n",
      " 0.1508\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 861000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.197501e+01/ 轮得分 4.99\n",
      "损失函数： Variable containing:\n",
      " 0.2048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 862000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.981626e+00/ 轮得分 4.99\n",
      "损失函数： Variable containing:\n",
      " 0.1604\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 863000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.592104e+00/ 轮得分 5.00\n",
      "损失函数： Variable containing:\n",
      " 0.1225\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 864000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.415452e+01/ 轮得分 5.04\n",
      "损失函数： Variable containing:\n",
      " 0.1137\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 865000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.278087e+01/ 轮得分 5.02\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.4671\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 866000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 6.945290e+00/ 轮得分 4.99\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.3363\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 867000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.254064e+01/ 轮得分 4.98\n",
      "损失函数： Variable containing:\n",
      " 0.2248\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 868000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.105467e+01/ 轮得分 4.94\n",
      "损失函数： Variable containing:\n",
      " 0.8397\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 869000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.395898e+01/ 轮得分 4.95\n",
      "损失函数： Variable containing:\n",
      " 0.1265\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 870000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.268955e+00/ 轮得分 4.95\n",
      "损失函数： Variable containing:\n",
      " 0.1032\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 871000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.888346e+00/ 轮得分 4.94\n",
      "损失函数： Variable containing:\n",
      " 0.3637\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 872000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.297927e+01/ 轮得分 4.96\n",
      "损失函数： Variable containing:\n",
      " 0.1173\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 873000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.086715e+01/ 轮得分 4.97\n",
      "损失函数： Variable containing:\n",
      " 0.1111\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 874000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.009759e+01/ 轮得分 4.99\n",
      "损失函数： Variable containing:\n",
      " 0.1286\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 875000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.069911e+01/ 轮得分 4.98\n",
      "损失函数： Variable containing:\n",
      " 0.2365\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 876000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.268688e+01/ 轮得分 5.01\n",
      "损失函数： Variable containing:\n",
      " 0.2379\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 877000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.924002e+00/ 轮得分 5.06\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.8584\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 878000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.022979e+01/ 轮得分 5.01\n",
      "损失函数： Variable containing:\n",
      " 0.1903\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 879000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.328265e+01/ 轮得分 5.01\n",
      "损失函数： Variable containing:\n",
      " 0.6113\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 880000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.488914e+00/ 轮得分 4.98\n",
      "损失函数： Variable containing:\n",
      " 0.2638\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 881000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.678046e+00/ 轮得分 4.98\n",
      "损失函数： Variable containing:\n",
      " 0.1082\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 882000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.032664e+01/ 轮得分 5.01\n",
      "损失函数： Variable containing:\n",
      " 0.2444\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 883000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.037910e+00/ 轮得分 5.00\n",
      "损失函数： Variable containing:\n",
      " 0.3588\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 884000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 4.748546e+00/ 轮得分 5.04\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.5980\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 885000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.143884e+01/ 轮得分 4.98\n",
      "损失函数： Variable containing:\n",
      " 0.2793\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 886000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.261671e+01/ 轮得分 4.95\n",
      "损失函数： Variable containing:\n",
      " 0.1268\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 887000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.139855e+01/ 轮得分 4.91\n",
      "损失函数： Variable containing:\n",
      " 0.1418\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 888000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.925035e+00/ 轮得分 4.88\n",
      "损失函数： Variable containing:\n",
      " 1.2923\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 889000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.021933e+01/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      " 0.1741\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 890000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.001766e+01/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      " 0.8253\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 891000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.465663e+00/ 轮得分 4.86\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.1753\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 892000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 4.703556e+00/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      " 0.3381\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 893000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.182618e+01/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      " 0.4834\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 894000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.026322e+00/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.5102\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 895000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 6.208574e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.3304\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 896000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.019792e+01/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.2852\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 897000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.839162e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.2047\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 898000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 -1/ Q_MAX 2.547814e+00/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 1.1232\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 899000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.055230e+00/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.2484\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 900000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.274692e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.5943\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 901000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.214446e+01/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.7140\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 902000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 -1/ Q_MAX 4.492412e+00/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.1469\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 903000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 6.069330e+00/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.1082\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 904000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.240364e+00/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.3695\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 905000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.234305e+00/ 轮得分 4.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      " 0.1708\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 906000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 6.837947e+00/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.3183\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 907000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.090198e+01/ 轮得分 4.61\n",
      "损失函数： Variable containing:\n",
      " 0.2723\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 908000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.037689e+01/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.8681\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 909000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.208414e+01/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      " 0.2285\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 910000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.744252e+00/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      " 0.3360\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 911000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 2.380037e+00/ 轮得分 4.56\n",
      "损失函数： Variable containing:\n",
      " 0.4812\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 912000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 -1/ Q_MAX 5.715233e+00/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.9318\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 913000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.456672e+00/ 轮得分 4.55\n",
      "损失函数： Variable containing:\n",
      " 0.1613\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 914000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.227044e+01/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      " 0.1294\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 915000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.120419e+01/ 轮得分 4.56\n",
      "损失函数： Variable containing:\n",
      " 0.2310\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 916000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.469115e+00/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.7551\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 917000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.213500e+01/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      " 0.8224\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 918000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.073463e+01/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.4153\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 919000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.198431e+01/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      " 0.2504\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 920000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.224161e+00/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.1600\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 921000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.231730e+01/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8877\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 922000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.585874e+00/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      " 0.1226\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 923000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.212162e+01/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      " 0.1449\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 924000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.127583e+01/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.1960\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 925000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.434568e+00/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.2184\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 926000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 7.866240e+00/ 轮得分 4.48\n",
      "损失函数： Variable containing:\n",
      " 0.2173\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 927000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.701335e+00/ 轮得分 4.56\n",
      "损失函数： Variable containing:\n",
      " 0.1949\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 928000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.000227e+01/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      " 0.2512\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 929000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.419100e+00/ 轮得分 4.52\n",
      "损失函数： Variable containing:\n",
      " 0.1224\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 930000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.172994e+01/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.8978\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 931000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.211735e+01/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      " 0.1476\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 932000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.342036e+01/ 轮得分 4.48\n",
      "损失函数： Variable containing:\n",
      " 0.3341\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 933000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.353485e+00/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.2613\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 934000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.086278e+01/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      " 0.1539\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 935000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.529339e+00/ 轮得分 4.47\n",
      "损失函数： Variable containing:\n",
      " 0.1380\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 936000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.303698e+01/ 轮得分 4.45\n",
      "损失函数： Variable containing:\n",
      " 0.1237\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 937000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 6.282944e+00/ 轮得分 4.46\n",
      "损失函数： Variable containing:\n",
      " 0.8604\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 938000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.187513e+01/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.1118\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 939000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.454515e+01/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.2651\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 940000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.283923e+01/ 轮得分 4.48\n",
      "损失函数： Variable containing:\n",
      " 0.1938\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 941000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.884412e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      " 0.6352\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 942000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.053459e+01/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.4520\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 943000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.040216e+01/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      " 0.2079\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 944000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.015096e+01/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      " 0.1145\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 945000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.208495e+01/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.2882\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 946000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.823459e+00/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.9846\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 947000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.287708e+01/ 轮得分 4.61\n",
      "损失函数： Variable containing:\n",
      " 0.2096\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 948000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.030356e+01/ 轮得分 4.59\n",
      "损失函数： Variable containing:\n",
      " 0.2101\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 949000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 1/ Q_MAX 8.465747e+00/ 轮得分 4.60\n",
      "损失函数： Variable containing:\n",
      " 0.1311\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 950000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.095188e+01/ 轮得分 4.60\n",
      "损失函数： Variable containing:\n",
      " 0.2265\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 951000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.308207e+01/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      " 0.1001\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 952000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.269945e+01/ 轮得分 4.60\n",
      "损失函数： Variable containing:\n",
      " 0.2038\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 953000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.333663e+01/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.2927\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 954000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.242392e+00/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      " 0.6348\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 955000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.150421e+01/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.6510\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 956000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.241160e+00/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.5364\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 957000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.186366e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.2305\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 958000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.010372e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.1476\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 959000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 5.826263e+00/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.2070\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 960000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.934421e+00/ 轮得分 4.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      " 0.1462\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 961000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.116785e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.3450\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 962000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.931078e+00/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.1509\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 963000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.655201e+00/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.1172\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 964000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.084246e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 0.2924\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 965000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.260400e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.7557\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 966000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.185289e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.0485\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 967000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.200773e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 0.4069\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 968000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.836591e+00/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.2809\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 969000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.456404e+00/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.1271\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 970000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.890373e+00/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.1684\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 971000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.298734e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.5339\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 972000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.023530e+01/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.2563\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 973000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.185674e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.1258\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 974000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.063752e+01/ 轮得分 4.60\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.5982\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 975000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.067732e+00/ 轮得分 4.60\n",
      "损失函数： Variable containing:\n",
      " 0.1525\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 976000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.436179e+00/ 轮得分 4.61\n",
      "损失函数： Variable containing:\n",
      " 0.4167\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 977000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.075051e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7907\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 978000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.193311e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.5233\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 979000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.711119e+00/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      " 0.2796\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 980000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.019379e+01/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.2123\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 981000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.687336e+00/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.1866\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 982000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.260755e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.1163\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 983000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.153943e+01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.2359\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 984000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.221593e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.1848\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 985000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.206964e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.0253\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 986000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.186439e+01/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.5172\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 987000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.154750e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.4412\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 988000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.169186e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.4205\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 989000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 1/ Q_MAX 1.275657e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.1232\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 990000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.204475e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 1.1801\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 991000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.810994e+00/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 0.3774\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 992000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.077902e+01/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.6229\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 993000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.274460e+01/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3477\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 994000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.114958e+01/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      " 0.1838\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 995000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.066533e+01/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      " 0.4499\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 996000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.054885e+01/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      " 0.1228\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 997000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.244477e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.3193\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 998000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.359004e+01/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      " 0.1470\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 999000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.756617e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.0720\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1000000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.418420e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.2441\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1001000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.260941e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.1846\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1002000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.346205e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.9089\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1003000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.289966e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.1209\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1004000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.176974e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.1650\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1005000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.198049e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 1.1687\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1006000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.219101e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 0.1697\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1007000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.342558e+01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.7395\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1008000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.242226e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.6884\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1009000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.256754e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.5963\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1010000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.349247e+01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.0655\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1011000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.254935e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.1428\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1012000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.364365e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.1555\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1013000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.297452e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.1107\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1014000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.333790e+01/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.2559\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间步 1015000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 -1/ Q_MAX 3.974961e-01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.2848\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1016000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.273372e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.2305\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1017000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.109874e+01/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.1560\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1018000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 5.890232e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.1611\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1019000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.291163e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.1030\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1020000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.025963e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.1851\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1021000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.462232e+01/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      " 1.2950\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1022000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.343206e+01/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      " 0.8296\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1023000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.308540e+01/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      " 0.2702\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1024000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.233915e+01/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      " 1.1412\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1025000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.545915e+01/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      " 0.1569\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1026000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.171505e+01/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      " 0.2168\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1027000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.300303e+01/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.0040\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1028000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.410754e+01/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      " 0.4152\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1029000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.313343e+01/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3742\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1030000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.155226e+01/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      " 0.1592\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1031000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.329754e+01/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      " 0.2527\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1032000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.053582e+01/ 轮得分 4.83\n",
      "损失函数： Variable containing:\n",
      " 0.1639\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1033000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.359038e+01/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.4239\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1034000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.353929e+01/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      " 0.1917\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1035000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.459154e+01/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      " 0.1498\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1036000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.529385e+00/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      " 0.1704\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1037000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.327657e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.7897\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1038000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.149455e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.5622\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1039000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.172358e+01/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      " 0.8074\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1040000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.495310e+01/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      " 0.2030\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1041000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.183336e+01/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      " 0.2307\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1042000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.400299e+01/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.7625\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1043000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.432361e+01/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      " 0.1714\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1044000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.547235e+00/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      " 0.1073\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1045000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.348528e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.1506\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1046000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.478038e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 2.8424\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1047000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.094311e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.3201\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1048000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.445234e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.4276\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1049000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.270299e+01/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      " 0.1245\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1050000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.091860e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.1231\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1051000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 8.765408e+00/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.7605\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1052000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 9.228464e+00/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      " 0.2035\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1053000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.318869e+01/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.6789\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1054000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.186266e+01/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      " 0.8412\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1055000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.166487e+01/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.3330\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1056000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.385004e+01/ 轮得分 4.79\n",
      "损失函数： Variable containing:\n",
      " 0.3008\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1057000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.358018e+01/ 轮得分 4.78\n",
      "损失函数： Variable containing:\n",
      " 0.2759\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1058000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.290229e+01/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      " 0.1052\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1059000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.396148e+01/ 轮得分 4.76\n",
      "损失函数： Variable containing:\n",
      " 0.5840\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1060000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.012823e+01/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      " 0.3524\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1061000/ 状态 explore/ Epsilon 0.07/ 行动 0/ 奖励 0.1/ Q_MAX 1.095619e+01/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      " 0.1437\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1062000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.214077e+01/ 轮得分 4.77\n",
      "损失函数： Variable containing:\n",
      " 0.1021\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1063000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.638342e+01/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      " 0.1965\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1064000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.227042e+01/ 轮得分 4.81\n",
      "损失函数： Variable containing:\n",
      " 0.1885\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1065000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.488785e+01/ 轮得分 4.84\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.6934\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1066000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.385323e+01/ 轮得分 4.85\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.8500\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1067000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 9.403782e+00/ 轮得分 4.87\n",
      "损失函数： Variable containing:\n",
      " 0.1548\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1068000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.353661e+01/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      " 0.1275\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1069000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.039299e+01/ 轮得分 4.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.8895\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1070000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.398713e+01/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      " 0.1072\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1071000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 -1/ Q_MAX 5.237525e+00/ 轮得分 4.82\n",
      "损失函数： Variable containing:\n",
      " 0.3359\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1072000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.413139e+01/ 轮得分 4.80\n",
      "损失函数： Variable containing:\n",
      " 0.1564\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1073000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.072175e+01/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      " 0.2084\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1074000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.369526e+01/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.0637\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1075000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.025746e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.7739\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1076000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.297347e+01/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      " 0.1156\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1077000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.315056e+01/ 轮得分 4.74\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.2104\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1078000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.443956e+01/ 轮得分 4.73\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.4717\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1079000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.423128e+01/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      " 0.1118\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1080000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.445405e+01/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      " 0.2172\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1081000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.231902e+01/ 轮得分 4.75\n",
      "损失函数： Variable containing:\n",
      " 0.1310\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1082000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.307765e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.8453\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1083000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.360803e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.2818\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1084000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.253778e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.3251\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1085000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 9.510182e+00/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.2783\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1086000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 9.843907e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.3602\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1087000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.406859e+01/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      " 0.1756\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1088000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.393515e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 2.2010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1089000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.348700e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 0.1183\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1090000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.494302e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.6870\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1091000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.410182e+01/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.1644\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1092000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.383509e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 0.2713\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1093000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.395664e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.2294\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1094000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.383104e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.0544\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1095000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.277082e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.4896\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1096000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.054649e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.1635\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1097000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.200643e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.1041\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1098000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.208795e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.7801\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1099000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.456977e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.6073\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1100000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.411202e+01/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      " 0.1325\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1101000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.174233e+01/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      " 0.2210\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1102000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.440296e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      " 0.1110\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1103000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.045397e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.8332\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1104000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.237141e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.1043\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1105000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.452241e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.1447\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1106000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 7.132699e+00/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 0.1193\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1107000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.129107e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.1813\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1108000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.127096e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.7471\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1109000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.387240e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.1693\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1110000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.445350e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.4129\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1111000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.263331e+01/ 轮得分 4.72\n",
      "损失函数： Variable containing:\n",
      " 0.1061\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1112000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.477319e+01/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.4414\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1113000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 1/ Q_MAX 4.497935e+00/ 轮得分 4.71\n",
      "损失函数： Variable containing:\n",
      " 0.2700\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1114000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.380569e+01/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.2932\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1115000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.194764e+01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.1025\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1116000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.326210e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 1.5260\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1117000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.339075e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.1035\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1118000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.398409e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.0152\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1119000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.234161e+01/ 轮得分 4.59\n",
      "损失函数： Variable containing:\n",
      " 0.2175\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1120000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.004435e+01/ 轮得分 4.59\n",
      "损失函数： Variable containing:\n",
      " 0.2581\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1121000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 8.978605e+00/ 轮得分 4.56\n",
      "损失函数： Variable containing:\n",
      " 0.1253\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1122000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.287456e+01/ 轮得分 4.58\n",
      "损失函数： Variable containing:\n",
      " 0.2721\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1123000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.101947e+01/ 轮得分 4.59\n",
      "损失函数： Variable containing:\n",
      " 0.6335\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间步 1124000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.331984e+01/ 轮得分 4.60\n",
      "损失函数： Variable containing:\n",
      " 0.8092\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1125000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.081886e+01/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      " 0.2168\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1126000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.311182e+01/ 轮得分 4.61\n",
      "损失函数： Variable containing:\n",
      " 0.2080\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1127000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.218831e+01/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      " 0.2847\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1128000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.158128e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.2024\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1129000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.333225e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.0312\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1130000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 9.883484e+00/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.3314\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1131000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.125932e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.2333\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1132000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.107055e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.8087\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1133000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.102512e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 0.1032\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1134000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.464462e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.1817\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1135000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.267970e+01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.9314\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1136000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.061281e+01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.9786\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1137000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.464287e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.4081\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1138000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 9.752889e+00/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.2281\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1139000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.299692e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 2.2132\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1140000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.399816e+01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.1240\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1141000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.574102e+01/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.1608\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1142000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.498311e+01/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.1889\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1143000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.382826e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 0.1146\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1144000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.411355e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.2062\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1145000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.299251e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.3892\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1146000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.192272e+01/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      " 0.3949\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1147000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.186286e+01/ 轮得分 4.59\n",
      "损失函数： Variable containing:\n",
      " 0.1345\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1148000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.184713e+01/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.8923\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1149000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.034995e+01/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      " 0.1759\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1150000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.337068e+01/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      " 0.1999\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1151000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.403500e+01/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3407\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1152000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 8.013847e+00/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.5992\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1153000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.333300e+01/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.2920\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1154000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.463659e+01/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      " 0.1173\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1155000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.265594e+01/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      " 0.1456\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1156000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.443108e+01/ 轮得分 4.46\n",
      "损失函数： Variable containing:\n",
      " 0.1109\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1157000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.168952e+01/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      " 0.1385\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1158000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.260733e+01/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      " 0.4430\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1159000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.192793e+01/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.1273\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1160000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.242206e+01/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      " 0.1379\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1161000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.303587e+01/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      " 0.3502\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1162000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.376209e+01/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.9316\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1163000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.379988e+01/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      " 1.8185\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1164000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.493442e+01/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      " 0.1101\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1165000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.130970e+01/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      " 0.1419\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1166000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.214421e+01/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      " 0.4255\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1167000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.512578e+01/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      " 0.8170\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1168000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.491980e+01/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      " 0.1174\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1169000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.412921e+01/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      " 0.1617\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1170000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.202827e+01/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      " 0.1085\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1171000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.263197e+01/ 轮得分 4.44\n",
      "损失函数： Variable containing:\n",
      " 0.1503\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1172000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.212518e+01/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.8026\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1173000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.203514e+01/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      " 0.1743\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1174000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.023786e+01/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      " 0.3378\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1175000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.240429e+01/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      " 0.1297\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1176000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.454421e+01/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      " 0.2825\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1177000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.298238e+01/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      " 0.1963\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1178000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.467409e+01/ 轮得分 4.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.6757\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1179000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.118649e+01/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      " 0.2680\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1180000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.462446e+01/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      " 0.2598\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1181000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.187605e+01/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      " 0.1294\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1182000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.539608e+01/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      " 0.1306\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1183000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.420711e+01/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      " 0.1690\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1184000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.659374e+01/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.6982\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1185000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.444086e+01/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      " 0.5754\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1186000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.051285e+01/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      " 0.3624\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1187000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.448265e+01/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      " 0.4592\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1188000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.549985e+01/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      " 0.3190\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1189000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.466471e+01/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      " 0.2246\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1190000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.455942e+01/ 轮得分 4.29\n",
      "损失函数： Variable containing:\n",
      " 0.6844\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1191000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.360967e+01/ 轮得分 4.29\n",
      "损失函数： Variable containing:\n",
      " 0.1260\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1192000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.411651e+01/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.7090\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1193000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.024206e+01/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.0891\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1194000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.219742e+01/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      " 0.2040\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1195000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.441309e+01/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      " 0.2258\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1196000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.474822e+01/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      " 0.1565\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1197000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.339231e+01/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.6705\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1198000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.199154e+01/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.1441\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1199000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.311460e+01/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      " 0.1576\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1200000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.365484e+01/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      " 0.2521\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1201000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.018749e+01/ 轮得分 4.44\n",
      "损失函数： Variable containing:\n",
      " 0.3671\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1202000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.376924e+01/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      " 0.7800\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1203000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 1/ Q_MAX 1.439046e+01/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      " 0.4925\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1204000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 1/ Q_MAX 1.400936e+01/ 轮得分 4.44\n",
      "损失函数： Variable containing:\n",
      " 0.2389\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1205000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.476501e+01/ 轮得分 4.47\n",
      "损失函数： Variable containing:\n",
      " 0.4213\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1206000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.342479e+01/ 轮得分 4.48\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.3287\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1207000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.481341e+01/ 轮得分 4.52\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.5570\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1208000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.049497e+01/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.2151\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1209000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.270799e+01/ 轮得分 4.48\n",
      "损失函数： Variable containing:\n",
      " 0.5302\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1210000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.437647e+01/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.4137\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1211000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.532284e+01/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.7209\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1212000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.000427e+01/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.1281\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1213000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.390789e+01/ 轮得分 4.50\n",
      "损失函数： Variable containing:\n",
      " 0.1839\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1214000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.499288e+01/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.2527\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1215000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.431768e+01/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      " 0.1435\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1216000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.297755e+01/ 轮得分 4.43\n",
      "损失函数： Variable containing:\n",
      " 0.2882\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1217000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.358146e+01/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      " 0.2471\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1218000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.507114e+01/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      " 0.1164\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1219000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.455421e+01/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      " 0.2900\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1220000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.271912e+01/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      " 0.2125\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1221000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.504195e+01/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      " 0.2139\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1222000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.476431e+01/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.9122\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1223000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 9.782649e+00/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      " 0.1818\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1224000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 -1/ Q_MAX 8.442558e+00/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      " 0.2379\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1225000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.351385e+01/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.1384\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1226000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.369118e+01/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      " 0.4229\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1227000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.491886e+01/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      " 0.1576\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1228000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.197409e+01/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      " 0.1234\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1229000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.524588e+01/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.9008\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1230000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.403395e+01/ 轮得分 4.29\n",
      "损失函数： Variable containing:\n",
      " 0.1176\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1231000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.297067e+01/ 轮得分 4.23\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.5981\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1232000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.300527e+01/ 轮得分 4.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      " 0.1478\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1233000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.365504e+01/ 轮得分 4.22\n",
      "损失函数： Variable containing:\n",
      " 1.0122\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1234000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.303169e+01/ 轮得分 4.19\n",
      "损失函数： Variable containing:\n",
      " 0.1361\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1235000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.433203e+01/ 轮得分 4.20\n",
      "损失函数： Variable containing:\n",
      " 0.1897\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1236000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 8.219258e+00/ 轮得分 4.19\n",
      "损失函数： Variable containing:\n",
      " 0.4028\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1237000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.181069e+01/ 轮得分 4.18\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7300\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1238000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.523502e+01/ 轮得分 4.16\n",
      "损失函数： Variable containing:\n",
      " 0.1044\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1239000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.232477e+01/ 轮得分 4.16\n",
      "损失函数： Variable containing:\n",
      " 0.1306\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1240000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.280885e+01/ 轮得分 4.15\n",
      "损失函数： Variable containing:\n",
      " 0.1009\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1241000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.563274e+01/ 轮得分 4.14\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.5005\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1242000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.327162e+01/ 轮得分 4.13\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.6265\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1243000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.400844e+01/ 轮得分 4.12\n",
      "损失函数： Variable containing:\n",
      " 0.1785\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1244000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.442610e+01/ 轮得分 4.12\n",
      "损失函数： Variable containing:\n",
      " 0.3348\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1245000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.219578e+01/ 轮得分 4.08\n",
      "损失函数： Variable containing:\n",
      " 0.1099\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1246000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.416068e+01/ 轮得分 4.08\n",
      "损失函数： Variable containing:\n",
      " 0.1363\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1247000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.086982e+01/ 轮得分 4.07\n",
      "损失函数： Variable containing:\n",
      " 0.4277\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1248000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.138611e+01/ 轮得分 3.98\n",
      "损失函数： Variable containing:\n",
      " 0.5047\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1249000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.391552e+01/ 轮得分 4.00\n",
      "损失函数： Variable containing:\n",
      " 0.1085\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1250000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.333700e+01/ 轮得分 3.99\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.3266\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1251000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.278483e+01/ 轮得分 4.01\n",
      "损失函数： Variable containing:\n",
      " 0.1413\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1252000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.464583e+01/ 轮得分 3.95\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.0025\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1253000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.515597e+01/ 轮得分 3.96\n",
      "损失函数： Variable containing:\n",
      " 0.2886\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1254000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.388793e+01/ 轮得分 3.96\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.2163\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1255000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.603682e+01/ 轮得分 3.96\n",
      "损失函数： Variable containing:\n",
      " 0.1682\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1256000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.283026e+01/ 轮得分 3.97\n",
      "损失函数： Variable containing:\n",
      " 0.1442\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1257000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.171175e+01/ 轮得分 3.96\n",
      "损失函数： Variable containing:\n",
      " 0.2873\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1258000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.488047e+01/ 轮得分 3.96\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.6649\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1259000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.427289e+01/ 轮得分 3.94\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.9978\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1260000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.311790e+01/ 轮得分 3.95\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3575\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1261000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.521879e+01/ 轮得分 3.97\n",
      "损失函数： Variable containing:\n",
      " 0.1511\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1262000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.562828e+01/ 轮得分 3.99\n",
      "损失函数： Variable containing:\n",
      " 0.7842\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1263000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.439343e+01/ 轮得分 3.99\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.9918\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1264000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.466656e+01/ 轮得分 3.98\n",
      "损失函数： Variable containing:\n",
      " 0.4736\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1265000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.574968e+01/ 轮得分 3.98\n",
      "损失函数： Variable containing:\n",
      " 0.2419\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1266000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.492933e+01/ 轮得分 4.00\n",
      "损失函数： Variable containing:\n",
      " 0.1632\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1267000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.248761e+01/ 轮得分 4.01\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.8204\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1268000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.427525e+01/ 轮得分 4.00\n",
      "损失函数： Variable containing:\n",
      " 0.3626\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1269000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.345202e+01/ 轮得分 4.01\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3488\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1270000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.495162e+01/ 轮得分 4.03\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.4348\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1271000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.370371e+01/ 轮得分 4.02\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.0993\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1272000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.363243e+01/ 轮得分 4.00\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.6425\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1273000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.411586e+01/ 轮得分 4.00\n",
      "损失函数： Variable containing:\n",
      " 0.1850\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1274000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.317928e+01/ 轮得分 3.99\n",
      "损失函数： Variable containing:\n",
      " 0.1150\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1275000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.483985e+01/ 轮得分 4.00\n",
      "损失函数： Variable containing:\n",
      " 0.1793\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1276000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.298802e+01/ 轮得分 3.98\n",
      "损失函数： Variable containing:\n",
      " 0.1606\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1277000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.400475e+01/ 轮得分 3.98\n",
      "损失函数： Variable containing:\n",
      " 0.2170\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1278000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.120354e+01/ 轮得分 3.98\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7015\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1279000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.411135e+01/ 轮得分 3.98\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.5473\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1280000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.158001e+01/ 轮得分 3.95\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.9341\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1281000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.539590e+01/ 轮得分 3.97\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.8009\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1282000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.426250e+01/ 轮得分 3.98\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.2260\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1283000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.190073e+01/ 轮得分 4.00\n",
      "损失函数： Variable containing:\n",
      " 0.4000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1284000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.447372e+01/ 轮得分 4.00\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  1.2835\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1285000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.425220e+01/ 轮得分 4.01\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3436\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "时间步 1286000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.259432e+01/ 轮得分 4.04\n",
      "损失函数： Variable containing:\n",
      " 0.1751\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1287000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.238904e+01/ 轮得分 4.04\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.2653\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1288000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 8.389017e+00/ 轮得分 4.02\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.5024\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1289000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.202069e+01/ 轮得分 4.03\n",
      "损失函数： Variable containing:\n",
      " 0.1419\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1290000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.380442e+01/ 轮得分 4.05\n",
      "损失函数： Variable containing:\n",
      " 0.9819\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1291000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.453267e+01/ 轮得分 4.05\n",
      "损失函数： Variable containing:\n",
      " 0.1925\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1292000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.586532e+01/ 轮得分 4.06\n",
      "损失函数： Variable containing:\n",
      " 0.6083\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1293000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.399226e+01/ 轮得分 4.05\n",
      "损失函数： Variable containing:\n",
      " 0.1113\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1294000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.289657e+01/ 轮得分 4.03\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.3260\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1295000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.439630e+01/ 轮得分 4.06\n",
      "损失函数： Variable containing:\n",
      " 1.6066\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1296000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.247454e+01/ 轮得分 4.08\n",
      "损失函数： Variable containing:\n",
      " 1.4376\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1297000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.366857e+01/ 轮得分 4.09\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.7500\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1298000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.390882e+01/ 轮得分 4.11\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.7531\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1299000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.277401e+01/ 轮得分 4.07\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.1374\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1300000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.418966e+01/ 轮得分 4.09\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.1835\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1301000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.053015e+01/ 轮得分 4.03\n",
      "损失函数： Variable containing:\n",
      " 0.2499\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1302000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.091818e+01/ 轮得分 4.05\n",
      "损失函数： Variable containing:\n",
      " 0.1544\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1303000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 -1/ Q_MAX 4.822335e+00/ 轮得分 4.05\n",
      "损失函数： Variable containing:\n",
      " 0.1142\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1304000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.334761e+01/ 轮得分 4.02\n",
      "损失函数： Variable containing:\n",
      " 0.1304\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1305000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.546921e+01/ 轮得分 4.03\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.1817\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1306000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.232848e+01/ 轮得分 4.05\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.4836\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1307000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.463234e+01/ 轮得分 4.09\n",
      "损失函数： Variable containing:\n",
      " 0.1579\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1308000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.561777e+01/ 轮得分 4.09\n",
      "损失函数： Variable containing:\n",
      " 0.5370\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1309000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.559110e+01/ 轮得分 4.08\n",
      "损失函数： Variable containing:\n",
      " 0.1320\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1310000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.524100e+01/ 轮得分 4.09\n",
      "损失函数： Variable containing:\n",
      " 0.1132\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1311000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.605250e+01/ 轮得分 4.09\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.9588\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1312000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.691060e+01/ 轮得分 4.09\n",
      "损失函数： Variable containing:\n",
      " 0.1056\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1313000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.498062e+01/ 轮得分 4.11\n",
      "损失函数： Variable containing:\n",
      " 0.1305\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1314000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.287364e+01/ 轮得分 4.13\n",
      "损失函数： Variable containing:\n",
      " 1.2030\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1315000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.339162e+01/ 轮得分 4.14\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.7929\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1316000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.427558e+01/ 轮得分 4.15\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.2692\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1317000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.587273e+01/ 轮得分 4.15\n",
      "损失函数： Variable containing:\n",
      " 0.1864\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1318000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.410438e+01/ 轮得分 4.14\n",
      "损失函数： Variable containing:\n",
      " 0.3047\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1319000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.672065e+01/ 轮得分 4.14\n",
      "损失函数： Variable containing:\n",
      " 0.2306\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1320000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.173335e+01/ 轮得分 4.17\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.7776\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1321000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.455067e+01/ 轮得分 4.18\n",
      "损失函数： Variable containing:\n",
      " 0.1091\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1322000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.697311e+01/ 轮得分 4.20\n",
      "损失函数： Variable containing:\n",
      " 0.2725\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1323000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.520923e+01/ 轮得分 4.20\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.7027\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1324000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 7.849274e+00/ 轮得分 4.20\n",
      "损失函数： Variable containing:\n",
      " 0.8265\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1325000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.499781e+01/ 轮得分 4.20\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.9924\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1326000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.531340e+01/ 轮得分 4.20\n",
      "损失函数： Variable containing:\n",
      " 0.1136\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1327000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 3.002403e+00/ 轮得分 4.22\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7930\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1328000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.331255e+01/ 轮得分 4.22\n",
      "损失函数： Variable containing:\n",
      " 0.1484\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1329000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.618724e+01/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      " 0.2528\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1330000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.519123e+01/ 轮得分 4.21\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.1835\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1331000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.478661e+01/ 轮得分 4.19\n",
      "损失函数： Variable containing:\n",
      " 0.1031\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1332000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.325176e+01/ 轮得分 4.20\n",
      "损失函数： Variable containing:\n",
      " 0.1587\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1333000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.193671e+01/ 轮得分 4.22\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.1906\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1334000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.461292e+01/ 轮得分 4.23\n",
      "损失函数： Variable containing:\n",
      " 0.1524\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1335000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.730280e+01/ 轮得分 4.20\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.5486\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1336000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 8.330060e+00/ 轮得分 4.21\n",
      "损失函数： Variable containing:\n",
      " 0.2440\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1337000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.270278e+01/ 轮得分 4.22\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.6390\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1338000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 4.328400e+00/ 轮得分 4.23\n",
      "损失函数： Variable containing:\n",
      " 0.1199\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1339000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.747496e+01/ 轮得分 4.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      " 0.7353\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1340000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.504031e+01/ 轮得分 4.23\n",
      "损失函数： Variable containing:\n",
      " 0.6633\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1341000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.665310e+01/ 轮得分 4.23\n",
      "损失函数： Variable containing:\n",
      " 1.4901\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1342000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.476888e+01/ 轮得分 4.24\n",
      "损失函数： Variable containing:\n",
      " 0.1609\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1343000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.329298e+01/ 轮得分 4.26\n",
      "损失函数： Variable containing:\n",
      " 0.1385\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1344000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.507757e+01/ 轮得分 4.24\n",
      "损失函数： Variable containing:\n",
      " 0.1577\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1345000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.181291e+01/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      " 0.3041\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1346000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.398442e+01/ 轮得分 4.24\n",
      "损失函数： Variable containing:\n",
      " 0.1560\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1347000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.273634e+01/ 轮得分 4.22\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.5744\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1348000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.330471e+01/ 轮得分 4.22\n",
      "损失函数： Variable containing:\n",
      " 0.1908\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1349000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.263480e+01/ 轮得分 4.21\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.7433\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1350000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.491555e+01/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.1958\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1351000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.499604e+01/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.0323\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1352000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.768733e+01/ 轮得分 4.29\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.5489\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1353000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.575253e+01/ 轮得分 4.29\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.1625\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1354000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.282172e+01/ 轮得分 4.27\n",
      "损失函数： Variable containing:\n",
      " 0.2698\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1355000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.684097e+01/ 轮得分 4.29\n",
      "损失函数： Variable containing:\n",
      " 0.1858\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1356000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.366657e+01/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      " 0.1272\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1357000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.122254e+01/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      " 0.9342\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1358000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.379819e+01/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      " 0.3678\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1359000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.465174e+01/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      " 0.1833\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1360000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.312324e+01/ 轮得分 4.27\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.0705\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1361000/ 状态 explore/ Epsilon 0.06/ 行动 0/ 奖励 0.1/ Q_MAX 1.694113e+01/ 轮得分 4.27\n",
      "损失函数： Variable containing:\n",
      " 0.1296\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1362000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.524207e+01/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      " 0.2369\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1363000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.637340e+01/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.9346\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1364000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.499402e+01/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      " 0.1902\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1365000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.013747e+01/ 轮得分 4.21\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.9162\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1366000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.423325e+01/ 轮得分 4.23\n",
      "损失函数： Variable containing:\n",
      " 0.5846\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1367000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.287341e+01/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.0358\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1368000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 5.988059e+00/ 轮得分 4.26\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.3917\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1369000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.600941e+01/ 轮得分 4.23\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.6822\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1370000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.548197e+01/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.0515\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1371000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.794974e+01/ 轮得分 4.26\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.6991\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1372000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX -1.172487e+00/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.2050\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1373000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.562779e+01/ 轮得分 4.27\n",
      "损失函数： Variable containing:\n",
      " 0.1762\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1374000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.703471e+01/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      " 0.4588\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1375000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.480015e+01/ 轮得分 4.31\n",
      "损失函数： Variable containing:\n",
      " 0.4258\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1376000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.785561e+01/ 轮得分 4.30\n",
      "损失函数： Variable containing:\n",
      " 0.1010\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1377000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.752147e+01/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      " 0.1423\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1378000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.572668e+01/ 轮得分 4.33\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.1811\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1379000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.491350e+01/ 轮得分 4.34\n",
      "损失函数： Variable containing:\n",
      " 0.5205\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1380000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.805968e+01/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      " 0.5094\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1381000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.150430e+01/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      " 1.1232\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1382000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.649384e+01/ 轮得分 4.37\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.8902\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1383000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.380739e+01/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      " 0.1881\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1384000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.678091e+01/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.6955\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1385000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.555374e+01/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      " 0.1521\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1386000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.053933e+01/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      " 0.1486\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1387000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.821539e+01/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      " 0.1031\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1388000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.378086e+01/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      " 1.1105\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1389000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.583215e+01/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      " 0.3086\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1390000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.705759e+01/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      " 0.1592\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1391000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.559493e+01/ 轮得分 4.38\n",
      "损失函数： Variable containing:\n",
      " 0.3566\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1392000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.691225e+01/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      " 0.1995\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1393000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.561146e+01/ 轮得分 4.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.3975\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1394000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.459947e+01/ 轮得分 4.46\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.6030\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1395000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.656328e+01/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      " 0.1835\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1396000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.474696e+01/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      " 0.2938\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1397000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.478822e+01/ 轮得分 4.52\n",
      "损失函数： Variable containing:\n",
      " 0.1016\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1398000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.626347e+01/ 轮得分 4.53\n",
      "损失函数： Variable containing:\n",
      " 0.1751\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1399000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.520194e+01/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      " 1.6411\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1400000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.800047e+01/ 轮得分 4.59\n",
      "损失函数： Variable containing:\n",
      " 0.1088\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1401000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.478143e+01/ 轮得分 4.58\n",
      "损失函数： Variable containing:\n",
      " 0.1337\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1402000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.627165e+01/ 轮得分 4.57\n",
      "损失函数： Variable containing:\n",
      " 0.1141\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1403000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.654837e+01/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      " 2.2753\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1404000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.711394e+01/ 轮得分 4.54\n",
      "损失函数： Variable containing:\n",
      " 0.1214\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1405000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.733872e+01/ 轮得分 4.56\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.0180\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1406000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.690641e+01/ 轮得分 4.60\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.1657\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1407000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 -1/ Q_MAX 9.138852e+00/ 轮得分 4.61\n",
      "损失函数： Variable containing:\n",
      " 0.1157\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1408000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.719437e+01/ 轮得分 4.60\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.7885\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1409000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.841278e+01/ 轮得分 4.59\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.3503\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1410000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.876432e+01/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      " 0.1252\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1411000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.974614e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.1792\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1412000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.547789e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.1999\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1413000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.839694e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.1904\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1414000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.597237e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.1020\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1415000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.696577e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.1296\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1416000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.398677e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.1173\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1417000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 3.678239e+00/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.5332\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1418000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.525328e+01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.1081\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1419000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.224273e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.9166\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1420000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.286069e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.1173\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1421000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.282300e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.9909\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1422000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.666358e+01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.2016\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1423000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.694927e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.6274\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1424000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.439860e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.1146\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1425000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 1/ Q_MAX 1.197904e+01/ 轮得分 4.68\n",
      "损失函数： Variable containing:\n",
      " 0.4804\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1426000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.693166e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.1317\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1427000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.744474e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.5269\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1428000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.726198e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.1903\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1429000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.653884e+01/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      " 0.1465\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1430000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.748230e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.1279\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1431000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.675936e+01/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      " 0.2548\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1432000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.725254e+01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.1120\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1433000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.769274e+01/ 轮得分 4.69\n",
      "损失函数： Variable containing:\n",
      " 0.1228\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1434000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.569181e+01/ 轮得分 4.66\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.4657\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1435000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.796012e+01/ 轮得分 4.70\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.8636\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1436000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.652022e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.0617\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1437000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.681675e+01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.1014\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1438000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.776375e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      " 0.1413\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1439000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.323769e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.1154\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1440000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.571705e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.8636\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1441000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.838034e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.9233\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1442000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.483264e+01/ 轮得分 4.63\n",
      "损失函数： Variable containing:\n",
      " 0.1200\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1443000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.828234e+01/ 轮得分 4.64\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  2.5841\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1444000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.506907e+01/ 轮得分 4.65\n",
      "损失函数： Variable containing:\n",
      " 0.5997\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1445000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.890898e+01/ 轮得分 4.67\n",
      "损失函数： Variable containing:\n",
      " 0.1223\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1446000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.795605e+01/ 轮得分 4.62\n",
      "损失函数： Variable containing:\n",
      " 0.2828\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1447000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.696267e+01/ 轮得分 4.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "损失函数： Variable containing:\n",
      " 0.1901\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1448000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.694737e+01/ 轮得分 4.51\n",
      "损失函数： Variable containing:\n",
      " 0.1886\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1449000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.728972e+01/ 轮得分 4.49\n",
      "损失函数： Variable containing:\n",
      " 0.2530\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1450000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.658863e+01/ 轮得分 4.45\n",
      "损失函数： Variable containing:\n",
      " 0.3170\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1451000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.470767e+01/ 轮得分 4.42\n",
      "损失函数： Variable containing:\n",
      " 0.2446\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1452000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.786950e+01/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.2246\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1453000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.741850e+01/ 轮得分 4.39\n",
      "损失函数： Variable containing:\n",
      " 0.1952\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1454000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.637932e+01/ 轮得分 4.40\n",
      "损失函数： Variable containing:\n",
      " 0.1931\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1455000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.584732e+01/ 轮得分 4.41\n",
      "损失函数： Variable containing:\n",
      " 0.6026\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1456000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.463280e+01/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      " 0.1753\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1457000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 1/ Q_MAX 1.635700e+01/ 轮得分 4.36\n",
      "损失函数： Variable containing:\n",
      " 0.1487\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1458000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.612537e+01/ 轮得分 4.35\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.0333\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1459000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.691496e+01/ 轮得分 4.32\n",
      "损失函数： Variable containing:\n",
      " 0.1148\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1460000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.466806e+01/ 轮得分 4.27\n",
      "损失函数： Variable containing:\n",
      " 0.8715\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1461000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.040201e+01/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.9157\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1462000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.299781e+01/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      " 0.7269\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1463000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.742583e+01/ 轮得分 4.22\n",
      "损失函数： Variable containing:\n",
      " 0.1085\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1464000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.371568e+01/ 轮得分 4.21\n",
      "损失函数： Variable containing:\n",
      " 0.1608\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1465000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.468859e+01/ 轮得分 4.23\n",
      "损失函数： Variable containing:\n",
      " 0.2354\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1466000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.702446e+01/ 轮得分 4.24\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.9354\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1467000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.898382e+01/ 轮得分 4.24\n",
      "损失函数： Variable containing:\n",
      " 0.1688\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1468000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.300914e+01/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      " 0.2738\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1469000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.811322e+01/ 轮得分 4.26\n",
      "损失函数： Variable containing:\n",
      " 0.1763\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1470000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.772765e+01/ 轮得分 4.25\n",
      "损失函数： Variable containing:\n",
      " 0.9846\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1471000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.437825e+01/ 轮得分 4.24\n",
      "损失函数： Variable containing:\n",
      " 0.1525\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1472000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.463479e+01/ 轮得分 4.27\n",
      "损失函数： Variable containing:\n",
      " 0.1994\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1473000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 -1/ Q_MAX 5.315584e+00/ 轮得分 4.28\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  6.6655\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1474000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.617879e+01/ 轮得分 4.26\n",
      "损失函数： Variable containing:\n",
      " 0.1684\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1475000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.549612e+01/ 轮得分 4.23\n",
      "损失函数： Variable containing:\n",
      " 0.3286\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1476000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.438738e+01/ 轮得分 4.23\n",
      "损失函数： Variable containing:\n",
      " 0.1618\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1477000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.611765e+01/ 轮得分 4.24\n",
      "损失函数： Variable containing:\n",
      " 0.2830\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1478000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.613704e+01/ 轮得分 4.24\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  4.7094\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1479000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.694526e+01/ 轮得分 4.17\n",
      "损失函数： Variable containing:\n",
      " 0.1215\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1480000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.735994e+01/ 轮得分 4.13\n",
      "损失函数： Variable containing:\n",
      " 0.1196\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1481000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.523408e+01/ 轮得分 4.09\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  5.1437\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1482000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.358241e+01/ 轮得分 4.09\n",
      "损失函数： Variable containing:\n",
      " 0.3994\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1483000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.381103e+01/ 轮得分 4.05\n",
      "损失函数： Variable containing:\n",
      " 0.1333\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1484000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.879472e+01/ 轮得分 4.06\n",
      "损失函数： Variable containing:\n",
      " 0.1000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1485000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.519205e+01/ 轮得分 4.06\n",
      "损失函数： Variable containing:\n",
      " 0.1017\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1486000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.659440e+01/ 轮得分 4.07\n",
      "损失函数： Variable containing:\n",
      " 0.3294\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1487000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.604937e+01/ 轮得分 4.05\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  8.8702\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1488000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.367863e+01/ 轮得分 4.03\n",
      "损失函数： Variable containing:\n",
      " 0.3692\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1489000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.319905e+01/ 轮得分 4.04\n",
      "损失函数： Variable containing:\n",
      " 0.1713\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1490000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.327721e+01/ 轮得分 4.05\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  9.0001\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1491000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.382984e+01/ 轮得分 4.01\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  3.5108\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1492000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.224936e+01/ 轮得分 3.98\n",
      "损失函数： Variable containing:\n",
      " 0.1836\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "时间步 1493000/ 状态 explore/ Epsilon 0.05/ 行动 0/ 奖励 0.1/ Q_MAX 1.544460e+01/ 轮得分 4.01\n",
      "损失函数： Variable containing:\n",
      "1.00000e-02 *\n",
      "  7.5796\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 记录每轮平均得分的容器\n",
    "scores = []\n",
    "all_turn_scores = []\n",
    "while \"flappy bird\" != \"angry bird\":\n",
    "    # 开始游戏循环\n",
    "    ######################################################\n",
    "    ##########首先，按照贪婪策略选择一个行动 ##################\n",
    "    s = torch.from_numpy(s_t).type(torch.FloatTensor).requires_grad_(False)\n",
    "    s = s.cuda() if use_cuda else s\n",
    "    s = s.view(-1, s.size()[0], s.size()[1], s.size()[2])\n",
    "    # 获取当前时刻的游戏画面，输入到神经网络中\n",
    "    readout, h_fc1 = net(s)\n",
    "    # 神经网络产生的输出为readout：选择每一个行动的预期Q值\n",
    "    readout = readout.cpu() if use_cuda else readout\n",
    "    # readout为一个二维向量，分别对应每一个动作的预期Q值\n",
    "    readout_t = readout.data.numpy()[0]\n",
    "\n",
    "    # 按照epsilon贪婪策略产生小鸟的行动，即以epsilon的概率随机输出行动或者以\n",
    "    # 1-epsilon的概率按照预期输出最大的Q值给出行动\n",
    "    a_t = np.zeros([ACTIONS])\n",
    "    action_index = 0\n",
    "    if t % FRAME_PER_ACTION == 0:\n",
    "        # 如果当前帧可以行动，则\n",
    "        if random.random() <= epsilon:\n",
    "            # 产生随机行动\n",
    "            #print(\"----------Random Action----------\")\n",
    "            action_index = random.randrange(ACTIONS)\n",
    "        else:\n",
    "            # 选择神经网络判断的预期Q最大的行动\n",
    "            action_index = np.argmax(readout_t)\n",
    "        a_t[action_index] = 1\n",
    "    else:\n",
    "        a_t[0] = 1 # do nothing\n",
    "\n",
    "    # 模拟退火：让epsilon开始降低\n",
    "    if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "    ######################################################################### \n",
    "    ##########其次，将选择好的行动输入给游戏引擎，并得到下一帧的状态 ################### \n",
    "    x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
    "    # 返回的x_t1_colored为游戏画面，r_t为本轮的得分，terminal为游戏在本轮是否已经结束\n",
    "    \n",
    "    # 记录一下每一步的成绩\n",
    "    scores.append(r_t)\n",
    "    if terminal:\n",
    "        # 当游戏结束的时候，计算一下本轮的总成绩，并将总成绩存储到all_turn_scores中\n",
    "        all_turn_scores.append(sum(scores))\n",
    "        scores = []\n",
    "    \n",
    "    # 对游戏的原始画面做相应的处理，从而变成一张80*80的，朴素的（无背景画面）的图\n",
    "    x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n",
    "    x_t1 = np.reshape(x_t1, (1, 80, 80))\n",
    "    # 将当前帧的画面和前三帧的画面合并起来作为Agent获得的环境反馈结果\n",
    "    s_t1 = np.append(x_t1, s_t[:3, :, :], axis=0)\n",
    "    # 生成一个训练数据，分别将本帧的输入画面s_t,本帧的行动a_t，得到的环境回报r_t以及环境被转换的新状态s_t1存到D中\n",
    "    D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "    if len(D) > REPLAY_MEMORY:\n",
    "        # 如果D中的元素已满，则扔掉最老的一条训练数据\n",
    "        D.popleft()\n",
    "\n",
    "    ######################################################################### \n",
    "    ##########最后，当运行周期超过一定次数后开始训练神经网络 ################### \n",
    "    if t > OBSERVE:\n",
    "        # 从D中随机采样出一个batch的训练数据\n",
    "        minibatch = random.sample(D, BATCH)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 将这个batch中的s变量都分别存放到列表中\n",
    "        s_j_batch = [d[0] for d in minibatch]\n",
    "        a_batch = [d[1] for d in minibatch]\n",
    "        r_batch = [d[2] for d in minibatch]\n",
    "        s_j1_batch = [d[3] for d in minibatch]\n",
    "\n",
    "        # 接下来，要根据s_j1_batch，神经网络给出预估的未来Q值\n",
    "        \n",
    "        s = torch.tensor(np.array(s_j1_batch, dtype=float), dtype = torch.float, requires_grad = True)\n",
    "        s = s.cuda() if use_cuda else s\n",
    "        readout, h_fc1 = net(s)\n",
    "        readout = readout.cpu() if use_cuda else readout\n",
    "        readout_j1_batch = readout.data.numpy()\n",
    "        # readout_j1_batch存储了一个minibatch中的所有未来一步的Q预估值\n",
    "        # 根据Q的预估值，当前的反馈r，以及游戏是否结束，更新待训练的目标函数值\n",
    "        y_batch = []\n",
    "        for i in range(0, len(minibatch)):\n",
    "            terminal = minibatch[i][4]\n",
    "            # 当游戏结束的时候，则用环境的反馈作为目标，否则用下一状态的Q值＋本期的环境反馈\n",
    "            if terminal:\n",
    "                y_batch.append(r_batch[i])\n",
    "            else:\n",
    "                y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
    "\n",
    "        # 开始梯度更新\n",
    "        y = torch.tensor(y_batch, dtype = torch.float)\n",
    "        a = torch.tensor(a_batch, dtype = torch.float)\n",
    "        s = torch.tensor(np.array(s_j_batch, dtype=float), dtype = torch.float)\n",
    "        if use_cuda:\n",
    "            y = y.cuda()\n",
    "            a = a.cuda()\n",
    "            s = s.cuda()\n",
    "        # 计算s_j_batch的Q值\n",
    "        readout, h_fc1 = net(s)\n",
    "        readout_action = readout.mul(a).sum(1)\n",
    "        # 根据s_j_batch下所选择的预估Q和目标y的Q值的差来作为损失函数训练网络\n",
    "        loss = criterion(readout_action, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if t % 1000 == 0:\n",
    "            print('损失函数：', loss)\n",
    "       \n",
    "\n",
    "    # 将状态更新一次，时间步＋1\n",
    "    s_t = s_t1\n",
    "    t += 1\n",
    "\n",
    "    # 每隔 10000 次循环，存储一下网络\n",
    "    if t % 10000 == 0:\n",
    "        torch.save(net, 'saving_nets/' + GAME + '-dqn' + str(t) + '.txt')\n",
    "    \n",
    "    # 状态信息的转化，基本分为Observe，explore和train三个阶段\n",
    "    # Observe没有训练，explore开始训练，并且开始模拟退火，train模拟退火结束\n",
    "    state = \"\"\n",
    "    if t <= OBSERVE:\n",
    "        state = \"observe\"\n",
    "    elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "        state = \"explore\"\n",
    "    else:\n",
    "        state = \"train\"\n",
    "        \n",
    "    # 打印当前运行的一些基本数据，分别输出到屏幕以及log文件中\n",
    "    if t % 1000 == 0:\n",
    "        sss = \"时间步 {}/ 状态 {}/ Epsilon {:.2f}/ 行动 {}/ 奖励 {}/ Q_MAX {:e}/ 轮得分 {:.2f}\".format(\n",
    "            t, state, epsilon, action_index, r_t, np.max(readout_t), np.mean(all_turn_scores[-1000:]))\n",
    "        print(sss)\n",
    "        f = open('log_file.txt', 'a')\n",
    "        f.write(sss + '\\n')\n",
    "        f.close()\n",
    "    # write info to files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117df8d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = open('final_log_file.txt', 'r')\n",
    "line = f.read().strip().split('\\n')\n",
    "values = []\n",
    "for ln in line:\n",
    "    segs = ln.split('/')\n",
    "    values.append(float(segs[-1].split(' ')[-1]))\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(values))*1000, values)\n",
    "plt.xlabel('Frames')\n",
    "plt.ylabel('Average Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#net = torch.load('saving_nets/' + GAME + '-dqn' + str(2876000) + '.txt')\n",
    "net = torch.load('final_model.mdl')\n",
    "FINAL_EPSILON = 0.0001 # epsilon的最终值\n",
    "BATCH = 32 # 每一个批次的数据记录条数\n",
    "FRAME_PER_ACTION = 1 # 每间隔多少时间完成一次有效动作的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-6053185c23ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2198\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m                     \u001b[0mdryrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2200\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2201\u001b[0m                 \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m                 \u001b[0mbbox_inches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0moriginal_dpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbefore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mafter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1144\u001b[0;31m                 renderer, self, dsu, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, dsu, suppress_composite)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdsu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbefore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mafter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2424\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2426\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, dsu, suppress_composite)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdsu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mbefore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mafter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1136\u001b[0;31m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1137\u001b[0m         ticklabelBoxes, ticklabelBoxes2 = self._get_tick_bboxes(ticks_to_draw,\n\u001b[1;32m   1138\u001b[0m                                                                 renderer)\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_update_ticks\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0minterval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_view_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0mtick_tups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_bounds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0;31m# handle inverted limits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0minterval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_view_interval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m         \u001b[0mtick_tups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_bounds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0;31m# handle inverted limits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/learning_pytorch/lib/python3.5/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36miter_ticks\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0mmajorTicks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_major_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajorLocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmajorLocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         majorLabels = [self.major.formatter(val, i)\n\u001b[0m\u001b[1;32m    916\u001b[0m                        for i, val in enumerate(majorLocs)]\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 开始在内存／GPU上定义一个网络\n",
    "use_cuda = torch.cuda.is_available() #检测本台机器中是否有GPU\n",
    "\n",
    "# 如果有GPU，就把神经网络全部搬到GPU内存中做运算\n",
    "net = net.cuda() if use_cuda else net\n",
    "\n",
    "# 开启一个游戏进程，开始与游戏引擎通话\n",
    "game_state = GameState()\n",
    "\n",
    "# 状态打印log记录位置\n",
    "#a_file = open(\"logs_\" + GAME + \"/readout.txt\", 'w')\n",
    "#h_file = open(\"logs_\" + GAME + \"/hidden.txt\", 'w')\n",
    "\n",
    "# 将游戏设置为初始状态，并获得一个80*80的游戏湖面\n",
    "do_nothing = np.zeros(ACTIONS)\n",
    "do_nothing[0] = 1\n",
    "x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "\n",
    "# 将初始的游戏画面叠加成4张作为神经网络的初始输入状态s_t\n",
    "s_t = np.stack((x_t, x_t, x_t, x_t), axis=0)\n",
    "\n",
    "# 设置初始的epsilon（采取随机行动的概率），并准备训练\n",
    "epsilon = FINAL_EPSILON\n",
    "t = 0# 记录每轮平均得分的容器\n",
    "scores = []\n",
    "all_turn_scores = []\n",
    "\n",
    "fig = plt.figure()\n",
    "axe = fig.add_subplot(111)\n",
    "dat = np.zeros((10, 10))\n",
    "img = axe.imshow(dat)\n",
    "while \"flappy bird\" != \"angry bird\":\n",
    "    # 开始游戏循环\n",
    "    ######################################################\n",
    "    ##########首先，按照贪婪策略选择一个行动 ##################\n",
    "    s = torch.from_numpy(s_t).type(torch.FloatTensor).requires_grad_(False)\n",
    "    s = s.cuda() if use_cuda else s\n",
    "    s = s.view(-1, s.size()[0], s.size()[1], s.size()[2])\n",
    "    # 获取当前时刻的游戏画面，输入到神经网络中\n",
    "    readout, h_fc1 = net(s)\n",
    "    # 神经网络产生的输出为readout：选择每一个行动的预期Q值\n",
    "    readout = readout.cpu() if use_cuda else readout\n",
    "    # readout为一个二维向量，分别对应每一个动作的预期Q值\n",
    "    readout_t = readout.data.numpy()[0]\n",
    "\n",
    "    # 按照epsilon贪婪策略产生小鸟的行动，即以epsilon的概率随机输出行动或者以\n",
    "    # 1-epsilon的概率按照预期输出最大的Q值给出行动\n",
    "    a_t = np.zeros([ACTIONS])\n",
    "    action_index = 0\n",
    "    if t % FRAME_PER_ACTION == 0:\n",
    "        # 如果当前帧可以行动，则\n",
    "        if random.random() <= epsilon:\n",
    "            # 产生随机行动\n",
    "            #print(\"----------Random Action----------\")\n",
    "            action_index = random.randrange(ACTIONS)\n",
    "        else:\n",
    "            # 选择神经网络判断的预期Q最大的行动\n",
    "            action_index = np.argmax(readout_t)\n",
    "        a_t[action_index] = 1\n",
    "    else:\n",
    "        a_t[0] = 1 # do nothing\n",
    "    ######################################################################### \n",
    "    ##########其次，将选择好的行动输入给游戏引擎，并得到下一帧的状态 ################### \n",
    "    x_t1_colored, r_t, terminal = game_state.frame_step(a_t)\n",
    "    # 返回的x_t1_colored为游戏画面，r_t为本轮的得分，terminal为游戏在本轮是否已经结束\n",
    "    \n",
    "    # 记录一下每一步的成绩\n",
    "    scores.append(r_t)\n",
    "    if terminal:\n",
    "        # 当游戏结束的时候，计算一下本轮的总成绩，并将总成绩存储到all_turn_scores中\n",
    "        all_turn_scores.append(sum(scores))\n",
    "        scores = []\n",
    "    \n",
    "    # 对游戏的原始画面做相应的处理，从而变成一张80*80的，朴素的（无背景画面）的图\n",
    "    x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n",
    "    x_t1 = np.reshape(x_t1, (1, 80, 80))\n",
    "    # 将当前帧的画面和前三帧的画面合并起来作为Agent获得的环境反馈结果\n",
    "    s_t1 = np.append(x_t1, s_t[:3, :, :], axis=0)\n",
    "    s_t = s_t1\n",
    "    t += 1\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    image = np.transpose(x_t1_colored, (1, 0, 2))\n",
    "    img.set_data(image)\n",
    "    img.autoscale()\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
